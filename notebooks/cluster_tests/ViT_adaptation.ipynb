{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcebangu\u001b[0m (\u001b[33mmegdecoding\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoImageProcessor, ViTForImageClassification, Trainer, TrainingArguments, TrainerCallback\n",
    "from datasets import Dataset, Image, ClassLabel\n",
    "import evaluate\n",
    "import wandb\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the paths\n",
    "label_path = os.path.join(os.getcwd(), \"scalograms_test/average_labels.csv\")\n",
    "image_path = os.path.join(os.getcwd(), \"scalograms_test/images_average\")\n",
    "#get the labels\n",
    "df = pd.read_csv(label_path)\n",
    "# attach paths to labels\n",
    "df['image'] = df['FileName'].apply(lambda x: os.path.join(image_path, x))\n",
    "df[\"label\"] = df[\"Label\"].astype(\"category\")\n",
    "#organize df\n",
    "df = df.drop(columns=[\"Label\", \"FileName\"])\n",
    "# set labels as a label class - might be a good idea to do a label2id and change the labels from ints. \n",
    "unique_labels = set(df[\"label\"])\n",
    "class_label = ClassLabel(names=list(unique_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872e60846d924cb5b816b10bfda27008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/324 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#format dataset\n",
    "dataset = Dataset.from_pandas(df).cast_column(\"image\", Image())\n",
    "dataset = dataset.cast_column(\"label\", class_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor object for images for the model\n",
    "processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\", use_fast=True)\n",
    "processor.size = {\"height\" : 384, \"width\" : 384} # reccomended to fine-tune with 384x384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f648b605b549029efa1ad221924468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/324 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform_for_model(example):\n",
    "    \"\"\"Transform the dataset into the format the model is expecting\"\"\"\n",
    "    example['pixel_values'] = [image.convert(\"RGB\") for image in example['image']]\n",
    "    example['pixel_values'] = processor(example['pixel_values'], return_tensors='pt')[\"pixel_values\"]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(transform_for_model, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label', 'pixel_values'],\n",
       "    num_rows: 324\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90/10\n",
    "dataset = dataset.train_test_split(test_size=0.1, stratify_by_column=\"label\", seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok it's probably best to just wrap the model in a custom class\n",
    "\n",
    "The original model is trained on 224x224, but the best practices are to finetune of 384x384. So we need to at least interpolate the positional encodings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MEGVisionTransformer(ViTForImageClassification):\n",
    "    \"\"\"Custom VIT wrapper that makes sure the positional encodings are interpolated\n",
    "    This inherits from the base ViTForImageClassification class, doesn't need any arguments\n",
    "    It adds the interpolate_pos_encoding to the forward method\n",
    "    It also adds a method that allows you to freeze/unfreeze layers:\n",
    "        You can either unfreeze the classifier only (around 2k trainable params) : 'classifier'\n",
    "        Or you can unfreeze the classifier and the attention heads (around 28m trainable params) : 'attention'\n",
    "        Or you can unfreeze the entire model (around 80m trainable params) 'all'\n",
    "        \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs) # parent init\n",
    "\n",
    "    def forward(self, pixel_values=None, labels=None, **kwargs):\n",
    "        \"\"\"Custom forward pass because we need positional interpolation\"\"\"\n",
    "        return super().forward(\n",
    "            pixel_values=pixel_values,\n",
    "            labels=labels,\n",
    "            interpolate_pos_encoding=True, # need to define custom forward method so that we can interpolate the encodings\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    def freeze_type(self, freeze_type=None):\n",
    "        if freeze_type not in [\"classifier\", \"attention\", \"all\"]:\n",
    "            raise ValueError('freeze_type is either classifier, attention, or all')\n",
    "\n",
    "        if freeze_type == \"classifier\":\n",
    "            self.classifier_only()\n",
    "        elif freeze_type == \"attention\":\n",
    "            self.mha_training()\n",
    "        elif freeze_type == 'all':\n",
    "            self.train_all()\n",
    "\n",
    "    ### layer freezing methods\n",
    "    def classifier_only(self):\n",
    "        \"\"\"this keeps only the classifier open to training\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            # Unfreeze if the parameter belongs to the classifier or an attention layer.\n",
    "            if \"classifier\" in name:\n",
    "              param.requires_grad = True\n",
    "              print(f\"Unfreezing {name}\")\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "                print(f\"Freezing {name}\")\n",
    "\n",
    "    def mha_training(self):\n",
    "        \"\"\"This keeps the classifier and attention heads open to training, following\n",
    "        Touvron et al.\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            # Unfreeze if the parameter belongs to the classifier or an attention layer.\n",
    "            if \"classifier\" in name or \"attention\" in name:\n",
    "                param.requires_grad = True\n",
    "                print(f\"Unfreezing {name}\")\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "                print(f\"Freezing {name}\")\n",
    "    \n",
    "    def train_all(self):\n",
    "        \"\"\"Full model training\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            param.requires_grad = True\n",
    "            print(f\"Unfreezing {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading the pretrained model\n",
    "model_checkpoint = \"google/vit-base-patch16-224\"\n",
    "# num_classes = len(unique_labels)\n",
    "# model = MEGVisionTransformer.from_pretrained(\n",
    "#     model_checkpoint,\n",
    "#     ignore_mismatched_sizes=True,\n",
    "#     num_labels=num_classes\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which parameters to fine-tune? Well, Touvron et al say to fine tune the MHA layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.freeze_type(\"attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_params = sum([p.numel() for p in model.parameters()])\n",
    "# trainable_params = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
    "\n",
    "# print(f\"{num_params = :,} | {trainable_params = :,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    \"\"\"This function gets the samples in the right format for the model\"\"\"\n",
    "    pixel_values = torch.stack([torch.tensor(example[\"pixel_values\"]) for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics(eval_pred, num_classes=3):\n",
    "    \"\"\"this functions handles the metric computation for the trainer\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1) #logits to class\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, \n",
    "                                       references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, \n",
    "                           references=labels, \n",
    "                           average='weighted',\n",
    "                           )\n",
    "    precision = precision_metric.compute(predictions=predictions, \n",
    "                                         references=labels, \n",
    "                                         average='weighted',\n",
    "                                         zero_division=0.0 #control null predicition\n",
    "                                         )\n",
    "    recall = recall_metric.compute(predictions=predictions, \n",
    "                                   references=labels, \n",
    "                                   average='weighted',\n",
    "                                   zero_division=0.0 #control null predicition\n",
    "                                   )\n",
    "\n",
    "    # specificity we will have to compute manuall unfortunately\n",
    "    cm = confusion_matrix(labels, predictions, labels=list(range(num_classes))) # classes remember to change this!\n",
    "    \n",
    "    specificity_per_class = []\n",
    "    \n",
    "    for i in range(num_classes): #num classes\n",
    "        # true negatives\n",
    "        TN = np.sum(cm) - (np.sum(cm[i, :]) + np.sum(cm[:, i]) - cm[i, i]) \n",
    "        FP = np.sum(cm[:, i]) - cm[i, i]\n",
    "\n",
    "        # compute specificity for each class and avoid dividing by 0\n",
    "        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "        specificity_per_class.append(specificity)\n",
    "    \n",
    "    avg_specificity = np.mean(specificity_per_class)\n",
    "\n",
    "    wandb.log({\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "        \"f1\": f1[\"f1\"],\n",
    "        \"precision\": precision[\"precision\"],\n",
    "        \"recall\": recall[\"recall\"],\n",
    "        \"specificity\": avg_specificity,\n",
    "    })\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "        \"f1\": f1[\"f1\"],\n",
    "        \"precision\": precision[\"precision\"],\n",
    "        \"recall\": recall[\"recall\"],\n",
    "        \"specificity\": avg_specificity,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoldEvalTrackingCallback(TrainerCallback):\n",
    "    \"\"\"This Callback allows the tracking of per-fold metrics, like loss. Because the way the optimizer works it wants all the folds to be\n",
    "    part of the same run, but we want to get the results from each fold.\"\"\"\n",
    "    def __init__(self, fold_number):\n",
    "        self.fold_number = fold_number\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs:\n",
    "            new_logs = {f\"fold_{self.fold_number}/{key}\": value for key, value in logs.items()}\n",
    "            wandb.log(new_logs, commit=False)\n",
    "\n",
    "\n",
    "class TrainMetricsCallback(TrainerCallback): #creds to sid8491 from Transformers forum\n",
    "    \"\"\"This callback allows you to track evaluation metrics on the train set, to make sure\n",
    "    its learning. Unfortunately, it invloves doing ANOTHER forward pass over the data, rather than capturing\n",
    "    the logits and doing the evaluations on the fly. for 10k samples, i don't think this is a problem, but this is NOT\n",
    "    a scallable solution\"\"\"\n",
    "    def __init__(self, trainer):\n",
    "        super().__init__()\n",
    "        self._trainer = trainer\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if control.should_evaluate:\n",
    "            control_copy = deepcopy(control)\n",
    "            self._trainer.evaluate(eval_dataset = self._trainer.train_dataset, metric_key_prefix=\"train\")\n",
    "            return control_copy\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def cross_validate_kfold(train_dataset, model_class, model_name, num_classes=3, config=None, freeze_type=None, k=10):\n",
    "    \"\"\"Train dataset has to be dataset[\"train\"]\"\"\"\n",
    "    \n",
    "    run=wandb.init(project=\"VIT-KFold-HyperSweep\")\n",
    "    config=wandb.config if config is None else config\n",
    "    group_name = f\"ViT_lr:{config.learning_rate}_optim:{config.optim}_sched:{config.lr_scheduler_type}_grads:{config.gradient_accumulation_steps}\"\n",
    "    # Optionally, set tags or name here:\n",
    "    run.name = group_name\n",
    "\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset)):\n",
    "        print(f\"Training Fold {fold + 1}/{k}...\")\n",
    "\n",
    "        # remember to reinstantiate the model!!! and freeze the layers for each fold\n",
    "        \n",
    "        # model code\n",
    "        model = model_class.from_pretrained(model_name, \n",
    "                                            ignore_mismatched_sizes=True,\n",
    "                                            num_labels=num_classes\n",
    "                                            )\n",
    "        #trainer handles devices\n",
    "        #apply the freeze type\n",
    "        model.freeze_type(freeze_type)\n",
    "\n",
    "        # splitting into train and val\n",
    "        train_subset = train_dataset.select(train_idx.tolist())\n",
    "        val_subset = train_dataset.select(val_idx.tolist())\n",
    "        training_args = TrainingArguments(\n",
    "            f\"{model_name}-finetune_test\",\n",
    "            seed=42,\n",
    "            remove_unused_columns=False,\n",
    "            eval_strategy=\"epoch\", #maybe epoch is better?\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=config.learning_rate, # take it from the wandb config\n",
    "            lr_scheduler_type=config.lr_scheduler_type, # take from config\n",
    "            optim=config.optim, # tune optimizer\n",
    "            gradient_accumulation_steps=config.gradient_accumulation_steps, #tune gradient accumulation\n",
    "            per_device_train_batch_size=1,\n",
    "            per_device_eval_batch_size=1, \n",
    "            num_train_epochs=5, # SUPER IMPORTANT\n",
    "            warmup_ratio=0.1,\n",
    "            logging_steps=1, # change to more later\n",
    "            metric_for_best_model='eval_loss',\n",
    "            report_to=\"wandb\",\n",
    "            push_to_hub=False,\n",
    "        )\n",
    "        trainer = Trainer(\n",
    "            model=model, \n",
    "            args=training_args,\n",
    "            tokenizer=processor,\n",
    "            train_dataset=train_subset,\n",
    "            eval_dataset=val_subset,\n",
    "            data_collator=collate_fn,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[FoldEvalTrackingCallback(fold+1)]\n",
    "        )\n",
    "        \n",
    "        trainer.add_callback(TrainMetricsCallback(trainer=trainer))\n",
    "        trainer.train()\n",
    "        eval_results = trainer.evaluate()\n",
    "\n",
    "        wandb.log({f\"fold_{fold+1}_eval_loss\": eval_results[\"eval_loss\"]})\n",
    "        fold_results.append(eval_results[\"eval_loss\"]) # is this really what we want to track?\n",
    "        \n",
    "    avg_eval_loss=np.mean(fold_results)\n",
    "    wandb.log({\"avg_eval_loss\": avg_eval_loss})\n",
    "    \n",
    "    run.finish()\n",
    "    return avg_eval_loss\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'] = dataset['train'].select(list(range(20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "\n",
    "# configuration for hyperparameter sweep\n",
    "sweep_config = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\"name\": \"avg_eval_loss\", \"goal\": \"minimize\"}, # we want to optimize the average eval loss accross folds\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\": {\"values\": [1e-5, 3e-5, 5e-5, 1e-4]}, # sweep learning rates (we'll see how many we can do)\n",
    "        \"lr_scheduler_type\": {\"values\": [\"linear\", \"cosine\", \"constant\"]},\n",
    "        \"optim\": {\"values\": [\"adamw_torch\", \"adamw_hf\", \"adafactor\"]}, # have to consider this some more\n",
    "        \"gradient_accumulation_steps\": {\"values\": [1, 4, 8]}, # does this really matter?\n",
    "        },\n",
    "        \"early_terminate\": {\n",
    "            \"type\": \"hyperband\", # stop runs early\n",
    "            \"min_iter\": 15,\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 7kgx687p\n",
      "Sweep URL: https://wandb.ai/megdecoding/VIT-KFold-HyperSweep/sweeps/7kgx687p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5zdj9hwc with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgradient_accumulation_steps: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_scheduler_type: linear\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: adamw_hf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'VIT-KFold-HyperSweep' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/ciprianbangu/Cogmaster/M2 Internship/BCI code/wandb/run-20250227_190140-5zdj9hwc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/megdecoding/VIT-KFold-HyperSweep/runs/5zdj9hwc' target=\"_blank\">earthy-sweep-1</a></strong> to <a href='https://wandb.ai/megdecoding/VIT-KFold-HyperSweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/megdecoding/VIT-KFold-HyperSweep/sweeps/7kgx687p' target=\"_blank\">https://wandb.ai/megdecoding/VIT-KFold-HyperSweep/sweeps/7kgx687p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/megdecoding/VIT-KFold-HyperSweep' target=\"_blank\">https://wandb.ai/megdecoding/VIT-KFold-HyperSweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/megdecoding/VIT-KFold-HyperSweep/sweeps/7kgx687p' target=\"_blank\">https://wandb.ai/megdecoding/VIT-KFold-HyperSweep/sweeps/7kgx687p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/megdecoding/VIT-KFold-HyperSweep/runs/5zdj9hwc' target=\"_blank\">https://wandb.ai/megdecoding/VIT-KFold-HyperSweep/runs/5zdj9hwc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Fold 1/2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MEGVisionTransformer were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing vit.embeddings.cls_token\n",
      "Freezing vit.embeddings.position_embeddings\n",
      "Freezing vit.embeddings.patch_embeddings.projection.weight\n",
      "Freezing vit.embeddings.patch_embeddings.projection.bias\n",
      "Freezing vit.encoder.layer.0.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.0.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.0.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.0.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.0.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.0.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.0.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.0.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.0.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.0.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.0.output.dense.weight\n",
      "Freezing vit.encoder.layer.0.output.dense.bias\n",
      "Freezing vit.encoder.layer.0.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.0.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.0.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.0.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.1.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.1.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.1.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.1.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.1.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.1.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.1.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.1.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.1.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.1.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.1.output.dense.weight\n",
      "Freezing vit.encoder.layer.1.output.dense.bias\n",
      "Freezing vit.encoder.layer.1.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.1.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.1.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.1.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.2.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.2.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.2.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.2.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.2.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.2.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.2.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.2.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.2.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.2.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.2.output.dense.weight\n",
      "Freezing vit.encoder.layer.2.output.dense.bias\n",
      "Freezing vit.encoder.layer.2.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.2.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.2.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.2.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.3.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.3.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.3.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.3.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.3.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.3.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.3.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.3.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.3.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.3.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.3.output.dense.weight\n",
      "Freezing vit.encoder.layer.3.output.dense.bias\n",
      "Freezing vit.encoder.layer.3.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.3.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.3.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.3.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.4.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.4.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.4.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.4.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.4.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.4.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.4.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.4.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.4.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.4.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.4.output.dense.weight\n",
      "Freezing vit.encoder.layer.4.output.dense.bias\n",
      "Freezing vit.encoder.layer.4.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.4.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.4.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.4.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.5.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.5.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.5.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.5.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.5.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.5.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.5.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.5.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.5.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.5.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.5.output.dense.weight\n",
      "Freezing vit.encoder.layer.5.output.dense.bias\n",
      "Freezing vit.encoder.layer.5.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.5.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.5.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.5.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.6.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.6.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.6.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.6.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.6.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.6.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.6.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.6.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.6.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.6.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.6.output.dense.weight\n",
      "Freezing vit.encoder.layer.6.output.dense.bias\n",
      "Freezing vit.encoder.layer.6.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.6.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.6.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.6.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.7.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.7.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.7.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.7.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.7.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.7.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.7.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.7.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.7.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.7.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.7.output.dense.weight\n",
      "Freezing vit.encoder.layer.7.output.dense.bias\n",
      "Freezing vit.encoder.layer.7.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.7.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.7.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.7.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.8.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.8.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.8.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.8.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.8.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.8.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.8.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.8.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.8.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.8.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.8.output.dense.weight\n",
      "Freezing vit.encoder.layer.8.output.dense.bias\n",
      "Freezing vit.encoder.layer.8.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.8.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.8.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.8.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.9.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.9.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.9.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.9.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.9.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.9.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.9.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.9.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.9.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.9.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.9.output.dense.weight\n",
      "Freezing vit.encoder.layer.9.output.dense.bias\n",
      "Freezing vit.encoder.layer.9.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.9.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.9.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.9.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.10.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.10.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.10.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.10.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.10.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.10.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.10.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.10.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.10.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.10.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.10.output.dense.weight\n",
      "Freezing vit.encoder.layer.10.output.dense.bias\n",
      "Freezing vit.encoder.layer.10.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.10.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.10.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.10.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.11.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.11.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.11.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.11.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.11.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.11.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.11.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.11.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.11.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.11.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.11.output.dense.weight\n",
      "Freezing vit.encoder.layer.11.output.dense.bias\n",
      "Freezing vit.encoder.layer.11.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.11.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.11.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.11.layernorm_after.bias\n",
      "Freezing vit.layernorm.weight\n",
      "Freezing vit.layernorm.bias\n",
      "Unfreezing classifier.weight\n",
      "Unfreezing classifier.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/neuroai/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr_scheduler_type' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optim' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a357263d77894e4abb5bfbada1f25493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6489, 'grad_norm': 15.175420761108398, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.1}\n",
      "{'loss': 0.8265, 'grad_norm': 17.016061782836914, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.2}\n",
      "{'loss': 0.7092, 'grad_norm': 16.111312866210938, 'learning_rate': 6e-06, 'epoch': 0.3}\n",
      "{'loss': 0.921, 'grad_norm': 18.984773635864258, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.4}\n",
      "{'loss': 0.639, 'grad_norm': 14.324466705322266, 'learning_rate': 1e-05, 'epoch': 0.5}\n",
      "{'loss': 1.2964, 'grad_norm': 24.309703826904297, 'learning_rate': 9.777777777777779e-06, 'epoch': 0.6}\n",
      "{'loss': 0.607, 'grad_norm': 14.573837280273438, 'learning_rate': 9.555555555555556e-06, 'epoch': 0.7}\n",
      "{'loss': 1.9719, 'grad_norm': 27.325204849243164, 'learning_rate': 9.333333333333334e-06, 'epoch': 0.8}\n",
      "{'loss': 1.8825, 'grad_norm': 25.389753341674805, 'learning_rate': 9.111111111111112e-06, 'epoch': 0.9}\n",
      "{'loss': 1.7113, 'grad_norm': 25.66259002685547, 'learning_rate': 8.888888888888888e-06, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9b130db5174a32a396372fb24ec622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': 1.1115542650222778, 'train_accuracy': 0.5, 'train_f1': 0.4, 'train_precision': 0.33333333333333337, 'train_recall': 0.5, 'train_specificity': 0.6296296296296297, 'train_runtime': 1.7549, 'train_samples_per_second': 5.698, 'train_steps_per_second': 5.698, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faddf8ded8954880b19a4a3de05d20b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4181541204452515, 'eval_accuracy': 0.5, 'eval_f1': 0.34615384615384615, 'eval_precision': 0.2777777777777778, 'eval_recall': 0.5, 'eval_specificity': 0.7222222222222223, 'eval_runtime': 1.7972, 'eval_samples_per_second': 5.564, 'eval_steps_per_second': 5.564, 'epoch': 1.0}\n",
      "{'loss': 0.8994, 'grad_norm': 18.70414924621582, 'learning_rate': 8.666666666666668e-06, 'epoch': 1.1}\n",
      "{'loss': 0.6237, 'grad_norm': 14.074369430541992, 'learning_rate': 8.444444444444446e-06, 'epoch': 1.2}\n",
      "{'loss': 1.9665, 'grad_norm': 27.318836212158203, 'learning_rate': 8.222222222222222e-06, 'epoch': 1.3}\n",
      "{'loss': 0.5969, 'grad_norm': 14.38106632232666, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.4}\n",
      "{'loss': 1.3148, 'grad_norm': 24.492656707763672, 'learning_rate': 7.77777777777778e-06, 'epoch': 1.5}\n",
      "{'loss': 0.6814, 'grad_norm': 15.653271675109863, 'learning_rate': 7.555555555555556e-06, 'epoch': 1.6}\n",
      "{'loss': 0.8006, 'grad_norm': 16.660770416259766, 'learning_rate': 7.333333333333333e-06, 'epoch': 1.7}\n",
      "{'loss': 1.704, 'grad_norm': 25.649120330810547, 'learning_rate': 7.111111111111112e-06, 'epoch': 1.8}\n",
      "{'loss': 0.6228, 'grad_norm': 14.713441848754883, 'learning_rate': 6.88888888888889e-06, 'epoch': 1.9}\n",
      "{'loss': 1.8722, 'grad_norm': 25.361026763916016, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95130f171d3b43af8db1cda4f2b315e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': 1.1030337810516357, 'train_accuracy': 0.5, 'train_f1': 0.4, 'train_precision': 0.33333333333333337, 'train_recall': 0.5, 'train_specificity': 0.6296296296296297, 'train_runtime': 1.7196, 'train_samples_per_second': 5.815, 'train_steps_per_second': 5.815, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb62e797f204e7a9fccd3cef4fc7c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.411024808883667, 'eval_accuracy': 0.5, 'eval_f1': 0.34615384615384615, 'eval_precision': 0.2777777777777778, 'eval_recall': 0.5, 'eval_specificity': 0.7222222222222223, 'eval_runtime': 1.7204, 'eval_samples_per_second': 5.812, 'eval_steps_per_second': 5.812, 'epoch': 2.0}\n",
      "{'loss': 0.6207, 'grad_norm': 14.672813415527344, 'learning_rate': 6.444444444444445e-06, 'epoch': 2.1}\n",
      "{'loss': 0.7956, 'grad_norm': 16.588382720947266, 'learning_rate': 6.222222222222223e-06, 'epoch': 2.2}\n",
      "{'loss': 1.8696, 'grad_norm': 25.353952407836914, 'learning_rate': 6e-06, 'epoch': 2.3}\n",
      "{'loss': 0.8774, 'grad_norm': 18.381887435913086, 'learning_rate': 5.777777777777778e-06, 'epoch': 2.4}\n",
      "{'loss': 1.6993, 'grad_norm': 25.64452362060547, 'learning_rate': 5.555555555555557e-06, 'epoch': 2.5}\n",
      "{'loss': 1.9562, 'grad_norm': 27.31292152404785, 'learning_rate': 5.333333333333334e-06, 'epoch': 2.6}\n",
      "{'loss': 0.6681, 'grad_norm': 15.41782283782959, 'learning_rate': 5.1111111111111115e-06, 'epoch': 2.7}\n",
      "{'loss': 0.6067, 'grad_norm': 13.779902458190918, 'learning_rate': 4.888888888888889e-06, 'epoch': 2.8}\n",
      "{'loss': 1.3375, 'grad_norm': 24.696796417236328, 'learning_rate': 4.666666666666667e-06, 'epoch': 2.9}\n",
      "{'loss': 0.5818, 'grad_norm': 14.081876754760742, 'learning_rate': 4.444444444444444e-06, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e73fc17e7449feab741d56ee3e4d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': 1.096724033355713, 'train_accuracy': 0.6, 'train_f1': 0.45, 'train_precision': 0.36, 'train_recall': 0.6, 'train_specificity': 0.6666666666666666, 'train_runtime': 1.7406, 'train_samples_per_second': 5.745, 'train_steps_per_second': 5.745, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92bcdf32d7b346f7a359ff3150cacfd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4052608013153076, 'eval_accuracy': 0.5, 'eval_f1': 0.34615384615384615, 'eval_precision': 0.2777777777777778, 'eval_recall': 0.5, 'eval_specificity': 0.7222222222222223, 'eval_runtime': 1.7173, 'eval_samples_per_second': 5.823, 'eval_steps_per_second': 5.823, 'epoch': 3.0}\n",
      "{'loss': 1.6945, 'grad_norm': 25.631290435791016, 'learning_rate': 4.222222222222223e-06, 'epoch': 3.1}\n",
      "{'loss': 0.788, 'grad_norm': 16.478614807128906, 'learning_rate': 4.000000000000001e-06, 'epoch': 3.2}\n",
      "{'loss': 1.3411, 'grad_norm': 24.728364944458008, 'learning_rate': 3.777777777777778e-06, 'epoch': 3.3}\n",
      "{'loss': 1.8608, 'grad_norm': 25.326915740966797, 'learning_rate': 3.555555555555556e-06, 'epoch': 3.4}\n",
      "{'loss': 0.6027, 'grad_norm': 13.70849609375, 'learning_rate': 3.3333333333333333e-06, 'epoch': 3.5}\n",
      "{'loss': 1.9499, 'grad_norm': 27.302040100097656, 'learning_rate': 3.1111111111111116e-06, 'epoch': 3.6}\n",
      "{'loss': 0.5789, 'grad_norm': 14.024446487426758, 'learning_rate': 2.888888888888889e-06, 'epoch': 3.7}\n",
      "{'loss': 0.8653, 'grad_norm': 18.196645736694336, 'learning_rate': 2.666666666666667e-06, 'epoch': 3.8}\n",
      "{'loss': 0.6088, 'grad_norm': 14.443242073059082, 'learning_rate': 2.4444444444444447e-06, 'epoch': 3.9}\n",
      "{'loss': 0.6599, 'grad_norm': 15.270853042602539, 'learning_rate': 2.222222222222222e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d9bfc5c3194fa8a9bef2ea82f2c8d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': 1.0935015678405762, 'train_accuracy': 0.6, 'train_f1': 0.45, 'train_precision': 0.36, 'train_recall': 0.6, 'train_specificity': 0.6666666666666666, 'train_runtime': 1.7216, 'train_samples_per_second': 5.809, 'train_steps_per_second': 5.809, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163502a370574b1c90298659374904f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4025793075561523, 'eval_accuracy': 0.5, 'eval_f1': 0.34615384615384615, 'eval_precision': 0.2777777777777778, 'eval_recall': 0.5, 'eval_specificity': 0.7222222222222223, 'eval_runtime': 1.7461, 'eval_samples_per_second': 5.727, 'eval_steps_per_second': 5.727, 'epoch': 4.0}\n",
      "{'loss': 1.858, 'grad_norm': 25.31893539428711, 'learning_rate': 2.0000000000000003e-06, 'epoch': 4.1}\n",
      "{'loss': 0.6075, 'grad_norm': 14.419218063354492, 'learning_rate': 1.777777777777778e-06, 'epoch': 4.2}\n",
      "{'loss': 0.6585, 'grad_norm': 15.245996475219727, 'learning_rate': 1.5555555555555558e-06, 'epoch': 4.3}\n",
      "{'loss': 1.3472, 'grad_norm': 24.78129005432129, 'learning_rate': 1.3333333333333334e-06, 'epoch': 4.4}\n",
      "{'loss': 0.7837, 'grad_norm': 16.416072845458984, 'learning_rate': 1.111111111111111e-06, 'epoch': 4.5}\n",
      "{'loss': 0.5988, 'grad_norm': 13.640379905700684, 'learning_rate': 8.88888888888889e-07, 'epoch': 4.6}\n",
      "{'loss': 0.8609, 'grad_norm': 18.13213539123535, 'learning_rate': 6.666666666666667e-07, 'epoch': 4.7}\n",
      "{'loss': 1.9479, 'grad_norm': 27.30428123474121, 'learning_rate': 4.444444444444445e-07, 'epoch': 4.8}\n",
      "{'loss': 1.6903, 'grad_norm': 25.625232696533203, 'learning_rate': 2.2222222222222224e-07, 'epoch': 4.9}\n",
      "{'loss': 0.5753, 'grad_norm': 13.953465461730957, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618a5a2618104e5f8c52a9a585136198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': 1.0922694206237793, 'train_accuracy': 0.6, 'train_f1': 0.45, 'train_precision': 0.36, 'train_recall': 0.6, 'train_specificity': 0.6666666666666666, 'train_runtime': 1.7404, 'train_samples_per_second': 5.746, 'train_steps_per_second': 5.746, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f2ca46f4894f5f98d524797dce6228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4017517566680908, 'eval_accuracy': 0.5, 'eval_f1': 0.34615384615384615, 'eval_precision': 0.2777777777777778, 'eval_recall': 0.5, 'eval_specificity': 0.7222222222222223, 'eval_runtime': 1.7799, 'eval_samples_per_second': 5.618, 'eval_steps_per_second': 5.618, 'epoch': 5.0}\n",
      "{'train_runtime': 28.2532, 'train_samples_per_second': 1.77, 'train_steps_per_second': 1.77, 'train_loss': 1.1037298560142517, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "682966026dd64cf28089c5d5d39c7b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Fold 2/2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MEGVisionTransformer were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing vit.embeddings.cls_token\n",
      "Freezing vit.embeddings.position_embeddings\n",
      "Freezing vit.embeddings.patch_embeddings.projection.weight\n",
      "Freezing vit.embeddings.patch_embeddings.projection.bias\n",
      "Freezing vit.encoder.layer.0.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.0.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.0.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.0.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.0.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.0.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.0.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.0.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.0.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.0.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.0.output.dense.weight\n",
      "Freezing vit.encoder.layer.0.output.dense.bias\n",
      "Freezing vit.encoder.layer.0.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.0.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.0.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.0.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.1.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.1.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.1.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.1.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.1.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.1.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.1.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.1.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.1.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.1.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.1.output.dense.weight\n",
      "Freezing vit.encoder.layer.1.output.dense.bias\n",
      "Freezing vit.encoder.layer.1.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.1.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.1.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.1.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.2.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.2.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.2.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.2.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.2.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.2.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.2.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.2.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.2.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.2.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.2.output.dense.weight\n",
      "Freezing vit.encoder.layer.2.output.dense.bias\n",
      "Freezing vit.encoder.layer.2.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.2.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.2.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.2.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.3.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.3.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.3.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.3.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.3.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.3.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.3.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.3.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.3.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.3.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.3.output.dense.weight\n",
      "Freezing vit.encoder.layer.3.output.dense.bias\n",
      "Freezing vit.encoder.layer.3.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.3.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.3.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.3.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.4.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.4.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.4.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.4.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.4.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.4.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.4.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.4.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.4.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.4.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.4.output.dense.weight\n",
      "Freezing vit.encoder.layer.4.output.dense.bias\n",
      "Freezing vit.encoder.layer.4.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.4.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.4.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.4.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.5.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.5.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.5.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.5.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.5.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.5.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.5.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.5.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.5.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.5.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.5.output.dense.weight\n",
      "Freezing vit.encoder.layer.5.output.dense.bias\n",
      "Freezing vit.encoder.layer.5.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.5.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.5.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.5.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.6.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.6.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.6.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.6.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.6.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.6.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.6.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.6.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.6.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.6.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.6.output.dense.weight\n",
      "Freezing vit.encoder.layer.6.output.dense.bias\n",
      "Freezing vit.encoder.layer.6.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.6.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.6.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.6.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.7.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.7.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.7.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.7.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.7.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.7.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.7.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.7.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.7.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.7.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.7.output.dense.weight\n",
      "Freezing vit.encoder.layer.7.output.dense.bias\n",
      "Freezing vit.encoder.layer.7.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.7.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.7.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.7.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.8.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.8.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.8.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.8.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.8.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.8.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.8.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.8.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.8.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.8.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.8.output.dense.weight\n",
      "Freezing vit.encoder.layer.8.output.dense.bias\n",
      "Freezing vit.encoder.layer.8.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.8.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.8.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.8.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.9.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.9.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.9.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.9.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.9.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.9.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.9.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.9.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.9.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.9.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.9.output.dense.weight\n",
      "Freezing vit.encoder.layer.9.output.dense.bias\n",
      "Freezing vit.encoder.layer.9.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.9.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.9.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.9.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.10.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.10.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.10.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.10.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.10.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.10.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.10.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.10.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.10.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.10.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.10.output.dense.weight\n",
      "Freezing vit.encoder.layer.10.output.dense.bias\n",
      "Freezing vit.encoder.layer.10.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.10.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.10.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.10.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.11.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.11.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.11.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.11.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.11.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.11.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.11.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.11.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.11.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.11.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.11.output.dense.weight\n",
      "Freezing vit.encoder.layer.11.output.dense.bias\n",
      "Freezing vit.encoder.layer.11.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.11.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.11.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.11.layernorm_after.bias\n",
      "Freezing vit.layernorm.weight\n",
      "Freezing vit.layernorm.bias\n",
      "Unfreezing classifier.weight\n",
      "Unfreezing classifier.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/neuroai/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr_scheduler_type' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optim' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de55b36d6c04dedb1f64c01a9e0dd71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8807, 'grad_norm': 18.228017807006836, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.1}\n",
      "{'loss': 1.0062, 'grad_norm': 19.848224639892578, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.2}\n",
      "{'loss': 1.2905, 'grad_norm': 24.113126754760742, 'learning_rate': 6e-06, 'epoch': 0.3}\n",
      "{'loss': 0.5356, 'grad_norm': 12.705487251281738, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.4}\n",
      "{'loss': 1.5625, 'grad_norm': 24.86301040649414, 'learning_rate': 1e-05, 'epoch': 0.5}\n",
      "{'loss': 0.4894, 'grad_norm': 11.795477867126465, 'learning_rate': 9.777777777777779e-06, 'epoch': 0.6}\n",
      "{'loss': 0.6396, 'grad_norm': 14.654577255249023, 'learning_rate': 9.555555555555556e-06, 'epoch': 0.7}\n",
      "{'loss': 1.3845, 'grad_norm': 23.97956085205078, 'learning_rate': 9.333333333333334e-06, 'epoch': 0.8}\n",
      "{'loss': 1.253, 'grad_norm': 22.888071060180664, 'learning_rate': 9.111111111111112e-06, 'epoch': 0.9}\n",
      "{'loss': 1.4743, 'grad_norm': 23.89535903930664, 'learning_rate': 8.888888888888888e-06, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516d7caa990440e58900fb59db1b7081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': 1.0472664833068848, 'train_accuracy': 0.5, 'train_f1': 0.3333333333333333, 'train_precision': 0.25, 'train_recall': 0.5, 'train_specificity': 0.6666666666666666, 'train_runtime': 1.7208, 'train_samples_per_second': 5.811, 'train_steps_per_second': 5.811, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e8924907c0483c9e903e55950e940b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1275652647018433, 'eval_accuracy': 0.3, 'eval_f1': 0.13846153846153847, 'eval_precision': 0.09, 'eval_recall': 0.3, 'eval_specificity': 0.6666666666666666, 'eval_runtime': 1.7596, 'eval_samples_per_second': 5.683, 'eval_steps_per_second': 5.683, 'epoch': 1.0}\n",
      "{'loss': 0.5292, 'grad_norm': 12.593332290649414, 'learning_rate': 8.666666666666668e-06, 'epoch': 1.1}\n",
      "{'loss': 1.5692, 'grad_norm': 24.91934585571289, 'learning_rate': 8.444444444444446e-06, 'epoch': 1.2}\n",
      "{'loss': 1.3811, 'grad_norm': 23.970802307128906, 'learning_rate': 8.222222222222222e-06, 'epoch': 1.3}\n",
      "{'loss': 0.6333, 'grad_norm': 14.55295467376709, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.4}\n",
      "{'loss': 0.4844, 'grad_norm': 11.700690269470215, 'learning_rate': 7.77777777777778e-06, 'epoch': 1.5}\n",
      "{'loss': 1.292, 'grad_norm': 24.17259407043457, 'learning_rate': 7.555555555555556e-06, 'epoch': 1.6}\n",
      "{'loss': 0.9854, 'grad_norm': 19.60185432434082, 'learning_rate': 7.333333333333333e-06, 'epoch': 1.7}\n",
      "{'loss': 1.4681, 'grad_norm': 23.8712100982666, 'learning_rate': 7.111111111111112e-06, 'epoch': 1.8}\n",
      "{'loss': 0.8607, 'grad_norm': 17.96133804321289, 'learning_rate': 6.88888888888889e-06, 'epoch': 1.9}\n",
      "{'loss': 1.2466, 'grad_norm': 22.85222816467285, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ecb9d2f8e0142038ac5326afd2d3f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': 1.0427796840667725, 'train_accuracy': 0.5, 'train_f1': 0.3333333333333333, 'train_precision': 0.25, 'train_recall': 0.5, 'train_specificity': 0.6666666666666666, 'train_runtime': 1.7824, 'train_samples_per_second': 5.61, 'train_steps_per_second': 5.61, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82aff14f01584c84839ebb6eda604fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1250560283660889, 'eval_accuracy': 0.3, 'eval_f1': 0.13846153846153847, 'eval_precision': 0.09, 'eval_recall': 0.3, 'eval_specificity': 0.6666666666666666, 'eval_runtime': 1.7591, 'eval_samples_per_second': 5.685, 'eval_steps_per_second': 5.685, 'epoch': 2.0}\n",
      "{'loss': 0.8587, 'grad_norm': 17.9343318939209, 'learning_rate': 6.444444444444445e-06, 'epoch': 2.1}\n",
      "{'loss': 0.9798, 'grad_norm': 19.53397560119629, 'learning_rate': 6.222222222222223e-06, 'epoch': 2.2}\n",
      "{'loss': 1.2442, 'grad_norm': 22.837846755981445, 'learning_rate': 6e-06, 'epoch': 2.3}\n",
      "{'loss': 0.526, 'grad_norm': 12.547428131103516, 'learning_rate': 5.777777777777778e-06, 'epoch': 2.4}\n",
      "{'loss': 1.4631, 'grad_norm': 23.855905532836914, 'learning_rate': 5.555555555555557e-06, 'epoch': 2.5}\n",
      "{'loss': 1.3733, 'grad_norm': 23.952470779418945, 'learning_rate': 5.333333333333334e-06, 'epoch': 2.6}\n",
      "{'loss': 1.2866, 'grad_norm': 24.157344818115234, 'learning_rate': 5.1111111111111115e-06, 'epoch': 2.7}\n",
      "{'loss': 1.5838, 'grad_norm': 25.0030574798584, 'learning_rate': 4.888888888888889e-06, 'epoch': 2.8}\n",
      "{'loss': 0.4808, 'grad_norm': 11.629853248596191, 'learning_rate': 4.666666666666667e-06, 'epoch': 2.9}\n",
      "{'loss': 0.6244, 'grad_norm': 14.41051197052002, 'learning_rate': 4.444444444444444e-06, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f08fd22e7e4b658cd2dc0322c8a572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': 1.0385234355926514, 'train_accuracy': 0.5, 'train_f1': 0.3333333333333333, 'train_precision': 0.25, 'train_recall': 0.5, 'train_specificity': 0.6666666666666666, 'train_runtime': 1.7284, 'train_samples_per_second': 5.786, 'train_steps_per_second': 5.786, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f24d4a74914250a4dc8381a4ba713e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1220834255218506, 'eval_accuracy': 0.3, 'eval_f1': 0.13846153846153847, 'eval_precision': 0.09, 'eval_recall': 0.3, 'eval_specificity': 0.6666666666666666, 'eval_runtime': 1.7317, 'eval_samples_per_second': 5.775, 'eval_steps_per_second': 5.775, 'epoch': 3.0}\n",
      "{'loss': 1.4575, 'grad_norm': 23.825977325439453, 'learning_rate': 4.222222222222223e-06, 'epoch': 3.1}\n",
      "{'loss': 0.9717, 'grad_norm': 19.435148239135742, 'learning_rate': 4.000000000000001e-06, 'epoch': 3.2}\n",
      "{'loss': 0.4803, 'grad_norm': 11.621099472045898, 'learning_rate': 3.777777777777778e-06, 'epoch': 3.3}\n",
      "{'loss': 1.2363, 'grad_norm': 22.7814998626709, 'learning_rate': 3.555555555555556e-06, 'epoch': 3.4}\n",
      "{'loss': 1.5876, 'grad_norm': 25.022693634033203, 'learning_rate': 3.3333333333333333e-06, 'epoch': 3.5}\n",
      "{'loss': 1.3672, 'grad_norm': 23.92259407043457, 'learning_rate': 3.1111111111111116e-06, 'epoch': 3.6}\n",
      "{'loss': 0.6224, 'grad_norm': 14.377535820007324, 'learning_rate': 2.888888888888889e-06, 'epoch': 3.7}\n",
      "{'loss': 0.5253, 'grad_norm': 12.545524597167969, 'learning_rate': 2.666666666666667e-06, 'epoch': 3.8}\n",
      "{'loss': 0.8474, 'grad_norm': 17.781076431274414, 'learning_rate': 2.4444444444444447e-06, 'epoch': 3.9}\n",
      "{'loss': 1.2812, 'grad_norm': 24.12569236755371, 'learning_rate': 2.222222222222222e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32409d972a08459ca909e4183d16399f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': 1.0364038944244385, 'train_accuracy': 0.5, 'train_f1': 0.3333333333333333, 'train_precision': 0.25, 'train_recall': 0.5, 'train_specificity': 0.6666666666666666, 'train_runtime': 1.7766, 'train_samples_per_second': 5.629, 'train_steps_per_second': 5.629, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1000962fdfe4e7fad775efb0803a2e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1208440065383911, 'eval_accuracy': 0.3, 'eval_f1': 0.13846153846153847, 'eval_precision': 0.09, 'eval_recall': 0.3, 'eval_specificity': 0.6666666666666666, 'eval_runtime': 1.8591, 'eval_samples_per_second': 5.379, 'eval_steps_per_second': 5.379, 'epoch': 4.0}\n",
      "{'loss': 1.2344, 'grad_norm': 22.768720626831055, 'learning_rate': 2.0000000000000003e-06, 'epoch': 4.1}\n",
      "{'loss': 0.8464, 'grad_norm': 17.76696014404297, 'learning_rate': 1.777777777777778e-06, 'epoch': 4.2}\n",
      "{'loss': 1.2807, 'grad_norm': 24.123355865478516, 'learning_rate': 1.5555555555555558e-06, 'epoch': 4.3}\n",
      "{'loss': 0.4792, 'grad_norm': 11.599104881286621, 'learning_rate': 1.3333333333333334e-06, 'epoch': 4.4}\n",
      "{'loss': 0.9664, 'grad_norm': 19.37066078186035, 'learning_rate': 1.111111111111111e-06, 'epoch': 4.5}\n",
      "{'loss': 1.5906, 'grad_norm': 25.038902282714844, 'learning_rate': 8.88888888888889e-07, 'epoch': 4.6}\n",
      "{'loss': 0.5246, 'grad_norm': 12.535374641418457, 'learning_rate': 6.666666666666667e-07, 'epoch': 4.7}\n",
      "{'loss': 1.3653, 'grad_norm': 23.919218063354492, 'learning_rate': 4.444444444444445e-07, 'epoch': 4.8}\n",
      "{'loss': 1.4529, 'grad_norm': 23.809743881225586, 'learning_rate': 2.2222222222222224e-07, 'epoch': 4.9}\n",
      "{'loss': 0.6202, 'grad_norm': 14.343040466308594, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eabe9c5fe3a74e2d8f714d76c59698ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': 1.0356286764144897, 'train_accuracy': 0.5, 'train_f1': 0.3333333333333333, 'train_precision': 0.25, 'train_recall': 0.5, 'train_specificity': 0.6666666666666666, 'train_runtime': 2.0719, 'train_samples_per_second': 4.826, 'train_steps_per_second': 4.826, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f957f65b9604b44b024dd5db4275144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.120453119277954, 'eval_accuracy': 0.3, 'eval_f1': 0.13846153846153847, 'eval_precision': 0.09, 'eval_recall': 0.3, 'eval_specificity': 0.6666666666666666, 'eval_runtime': 2.1474, 'eval_samples_per_second': 4.657, 'eval_steps_per_second': 4.657, 'epoch': 5.0}\n",
      "{'train_runtime': 28.9282, 'train_samples_per_second': 1.728, 'train_steps_per_second': 1.728, 'train_loss': 1.0424906867742538, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485a119f8071412489f58ae6bbf482d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td></td></tr><tr><td>avg_eval_loss</td><td></td></tr><tr><td>eval/accuracy</td><td></td></tr><tr><td>eval/f1</td><td></td></tr><tr><td>eval/loss</td><td></td></tr><tr><td>eval/precision</td><td></td></tr><tr><td>eval/recall</td><td></td></tr><tr><td>eval/runtime</td><td></td></tr><tr><td>eval/samples_per_second</td><td></td></tr><tr><td>eval/specificity</td><td></td></tr><tr><td>eval/steps_per_second</td><td></td></tr><tr><td>f1</td><td></td></tr><tr><td>fold_1/epoch</td><td></td></tr><tr><td>fold_1/eval_accuracy</td><td></td></tr><tr><td>fold_1/eval_f1</td><td></td></tr><tr><td>fold_1/eval_loss</td><td></td></tr><tr><td>fold_1/eval_precision</td><td></td></tr><tr><td>fold_1/eval_recall</td><td></td></tr><tr><td>fold_1/eval_runtime</td><td></td></tr><tr><td>fold_1/eval_samples_per_second</td><td></td></tr><tr><td>fold_1/eval_specificity</td><td></td></tr><tr><td>fold_1/eval_steps_per_second</td><td></td></tr><tr><td>fold_1/grad_norm</td><td></td></tr><tr><td>fold_1/learning_rate</td><td></td></tr><tr><td>fold_1/loss</td><td></td></tr><tr><td>fold_1/total_flos</td><td></td></tr><tr><td>fold_1/train_accuracy</td><td></td></tr><tr><td>fold_1/train_f1</td><td></td></tr><tr><td>fold_1/train_loss</td><td></td></tr><tr><td>fold_1/train_precision</td><td></td></tr><tr><td>fold_1/train_recall</td><td></td></tr><tr><td>fold_1/train_runtime</td><td></td></tr><tr><td>fold_1/train_samples_per_second</td><td></td></tr><tr><td>fold_1/train_specificity</td><td></td></tr><tr><td>fold_1/train_steps_per_second</td><td></td></tr><tr><td>fold_1_eval_loss</td><td></td></tr><tr><td>fold_2/epoch</td><td></td></tr><tr><td>fold_2/eval_accuracy</td><td></td></tr><tr><td>fold_2/eval_f1</td><td></td></tr><tr><td>fold_2/eval_loss</td><td></td></tr><tr><td>fold_2/eval_precision</td><td></td></tr><tr><td>fold_2/eval_recall</td><td></td></tr><tr><td>fold_2/eval_runtime</td><td></td></tr><tr><td>fold_2/eval_samples_per_second</td><td></td></tr><tr><td>fold_2/eval_specificity</td><td></td></tr><tr><td>fold_2/eval_steps_per_second</td><td></td></tr><tr><td>fold_2/grad_norm</td><td></td></tr><tr><td>fold_2/learning_rate</td><td></td></tr><tr><td>fold_2/loss</td><td></td></tr><tr><td>fold_2/total_flos</td><td></td></tr><tr><td>fold_2/train_accuracy</td><td></td></tr><tr><td>fold_2/train_f1</td><td></td></tr><tr><td>fold_2/train_loss</td><td></td></tr><tr><td>fold_2/train_precision</td><td></td></tr><tr><td>fold_2/train_recall</td><td></td></tr><tr><td>fold_2/train_runtime</td><td></td></tr><tr><td>fold_2/train_samples_per_second</td><td></td></tr><tr><td>fold_2/train_specificity</td><td></td></tr><tr><td>fold_2/train_steps_per_second</td><td></td></tr><tr><td>fold_2_eval_loss</td><td></td></tr><tr><td>precision</td><td></td></tr><tr><td>recall</td><td></td></tr><tr><td>specificity</td><td></td></tr><tr><td>train/epoch</td><td></td></tr><tr><td>train/global_step</td><td></td></tr><tr><td>train/grad_norm</td><td></td></tr><tr><td>train/learning_rate</td><td></td></tr><tr><td>train/loss</td><td></td></tr><tr><td>train/train_accuracy</td><td></td></tr><tr><td>train/train_f1</td><td></td></tr><tr><td>train/train_precision</td><td></td></tr><tr><td>train/train_recall</td><td></td></tr><tr><td>train/train_specificity</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.3</td></tr><tr><td>avg_eval_loss</td><td>1.2611</td></tr><tr><td>eval/accuracy</td><td>0.3</td></tr><tr><td>eval/f1</td><td>0.13846</td></tr><tr><td>eval/loss</td><td>1.12045</td></tr><tr><td>eval/precision</td><td>0.09</td></tr><tr><td>eval/recall</td><td>0.3</td></tr><tr><td>eval/runtime</td><td>2.4401</td></tr><tr><td>eval/samples_per_second</td><td>4.098</td></tr><tr><td>eval/specificity</td><td>0.66667</td></tr><tr><td>eval/steps_per_second</td><td>4.098</td></tr><tr><td>f1</td><td>0.13846</td></tr><tr><td>fold_1/epoch</td><td>5</td></tr><tr><td>fold_1/eval_accuracy</td><td>0.5</td></tr><tr><td>fold_1/eval_f1</td><td>0.34615</td></tr><tr><td>fold_1/eval_loss</td><td>1.40175</td></tr><tr><td>fold_1/eval_precision</td><td>0.27778</td></tr><tr><td>fold_1/eval_recall</td><td>0.5</td></tr><tr><td>fold_1/eval_runtime</td><td>1.7659</td></tr><tr><td>fold_1/eval_samples_per_second</td><td>5.663</td></tr><tr><td>fold_1/eval_specificity</td><td>0.72222</td></tr><tr><td>fold_1/eval_steps_per_second</td><td>5.663</td></tr><tr><td>fold_1/grad_norm</td><td>13.95347</td></tr><tr><td>fold_1/learning_rate</td><td>0</td></tr><tr><td>fold_1/loss</td><td>0.5753</td></tr><tr><td>fold_1/total_flos</td><td>1.13866801201152e+16</td></tr><tr><td>fold_1/train_accuracy</td><td>0.6</td></tr><tr><td>fold_1/train_f1</td><td>0.45</td></tr><tr><td>fold_1/train_loss</td><td>1.10373</td></tr><tr><td>fold_1/train_precision</td><td>0.36</td></tr><tr><td>fold_1/train_recall</td><td>0.6</td></tr><tr><td>fold_1/train_runtime</td><td>28.2532</td></tr><tr><td>fold_1/train_samples_per_second</td><td>1.77</td></tr><tr><td>fold_1/train_specificity</td><td>0.66667</td></tr><tr><td>fold_1/train_steps_per_second</td><td>1.77</td></tr><tr><td>fold_1_eval_loss</td><td>1.40175</td></tr><tr><td>fold_2/epoch</td><td>5</td></tr><tr><td>fold_2/eval_accuracy</td><td>0.3</td></tr><tr><td>fold_2/eval_f1</td><td>0.13846</td></tr><tr><td>fold_2/eval_loss</td><td>1.12045</td></tr><tr><td>fold_2/eval_precision</td><td>0.09</td></tr><tr><td>fold_2/eval_recall</td><td>0.3</td></tr><tr><td>fold_2/eval_runtime</td><td>2.4401</td></tr><tr><td>fold_2/eval_samples_per_second</td><td>4.098</td></tr><tr><td>fold_2/eval_specificity</td><td>0.66667</td></tr><tr><td>fold_2/eval_steps_per_second</td><td>4.098</td></tr><tr><td>fold_2/grad_norm</td><td>14.34304</td></tr><tr><td>fold_2/learning_rate</td><td>0</td></tr><tr><td>fold_2/loss</td><td>0.6202</td></tr><tr><td>fold_2/total_flos</td><td>1.13866801201152e+16</td></tr><tr><td>fold_2/train_accuracy</td><td>0.5</td></tr><tr><td>fold_2/train_f1</td><td>0.33333</td></tr><tr><td>fold_2/train_loss</td><td>1.04249</td></tr><tr><td>fold_2/train_precision</td><td>0.25</td></tr><tr><td>fold_2/train_recall</td><td>0.5</td></tr><tr><td>fold_2/train_runtime</td><td>28.9282</td></tr><tr><td>fold_2/train_samples_per_second</td><td>1.728</td></tr><tr><td>fold_2/train_specificity</td><td>0.66667</td></tr><tr><td>fold_2/train_steps_per_second</td><td>1.728</td></tr><tr><td>fold_2_eval_loss</td><td>1.12045</td></tr><tr><td>precision</td><td>0.09</td></tr><tr><td>recall</td><td>0.3</td></tr><tr><td>specificity</td><td>0.66667</td></tr><tr><td>total_flos</td><td>1.13866801201152e+16</td></tr><tr><td>train/epoch</td><td>5</td></tr><tr><td>train/global_step</td><td>50</td></tr><tr><td>train/grad_norm</td><td>14.34304</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>0.6202</td></tr><tr><td>train/train_accuracy</td><td>0.5</td></tr><tr><td>train/train_f1</td><td>0.33333</td></tr><tr><td>train/train_precision</td><td>0.25</td></tr><tr><td>train/train_recall</td><td>0.5</td></tr><tr><td>train/train_specificity</td><td>0.66667</td></tr><tr><td>train_loss</td><td>1.04249</td></tr><tr><td>train_runtime</td><td>28.9282</td></tr><tr><td>train_samples_per_second</td><td>1.728</td></tr><tr><td>train_steps_per_second</td><td>1.728</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ViT_lr:1e-05_optim:adamw_hf_sched:linear_grads:1</strong> at: <a href='https://wandb.ai/megdecoding/VIT-KFold-HyperSweep/runs/5zdj9hwc' target=\"_blank\">https://wandb.ai/megdecoding/VIT-KFold-HyperSweep/runs/5zdj9hwc</a><br> View project at: <a href='https://wandb.ai/megdecoding/VIT-KFold-HyperSweep' target=\"_blank\">https://wandb.ai/megdecoding/VIT-KFold-HyperSweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250227_190140-5zdj9hwc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: m6rikv8v with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgradient_accumulation_steps: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_scheduler_type: cosine\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptim: adamw_torch\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'VIT-KFold-HyperSweep' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/ciprianbangu/Cogmaster/M2 Internship/BCI code/wandb/run-20250227_190253-m6rikv8v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/megdecoding/VIT-KFold-HyperSweep/runs/m6rikv8v' target=\"_blank\">glamorous-sweep-2</a></strong> to <a href='https://wandb.ai/megdecoding/VIT-KFold-HyperSweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/megdecoding/VIT-KFold-HyperSweep/sweeps/7kgx687p' target=\"_blank\">https://wandb.ai/megdecoding/VIT-KFold-HyperSweep/sweeps/7kgx687p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/megdecoding/VIT-KFold-HyperSweep' target=\"_blank\">https://wandb.ai/megdecoding/VIT-KFold-HyperSweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/megdecoding/VIT-KFold-HyperSweep/sweeps/7kgx687p' target=\"_blank\">https://wandb.ai/megdecoding/VIT-KFold-HyperSweep/sweeps/7kgx687p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/megdecoding/VIT-KFold-HyperSweep/runs/m6rikv8v' target=\"_blank\">https://wandb.ai/megdecoding/VIT-KFold-HyperSweep/runs/m6rikv8v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Fold 1/2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MEGVisionTransformer were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing vit.embeddings.cls_token\n",
      "Freezing vit.embeddings.position_embeddings\n",
      "Freezing vit.embeddings.patch_embeddings.projection.weight\n",
      "Freezing vit.embeddings.patch_embeddings.projection.bias\n",
      "Freezing vit.encoder.layer.0.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.0.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.0.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.0.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.0.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.0.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.0.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.0.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.0.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.0.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.0.output.dense.weight\n",
      "Freezing vit.encoder.layer.0.output.dense.bias\n",
      "Freezing vit.encoder.layer.0.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.0.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.0.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.0.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.1.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.1.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.1.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.1.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.1.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.1.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.1.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.1.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.1.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.1.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.1.output.dense.weight\n",
      "Freezing vit.encoder.layer.1.output.dense.bias\n",
      "Freezing vit.encoder.layer.1.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.1.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.1.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.1.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.2.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.2.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.2.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.2.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.2.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.2.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.2.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.2.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.2.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.2.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.2.output.dense.weight\n",
      "Freezing vit.encoder.layer.2.output.dense.bias\n",
      "Freezing vit.encoder.layer.2.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.2.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.2.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.2.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.3.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.3.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.3.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.3.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.3.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.3.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.3.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.3.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.3.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.3.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.3.output.dense.weight\n",
      "Freezing vit.encoder.layer.3.output.dense.bias\n",
      "Freezing vit.encoder.layer.3.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.3.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.3.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.3.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.4.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.4.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.4.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.4.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.4.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.4.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.4.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.4.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.4.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.4.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.4.output.dense.weight\n",
      "Freezing vit.encoder.layer.4.output.dense.bias\n",
      "Freezing vit.encoder.layer.4.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.4.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.4.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.4.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.5.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.5.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.5.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.5.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.5.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.5.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.5.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.5.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.5.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.5.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.5.output.dense.weight\n",
      "Freezing vit.encoder.layer.5.output.dense.bias\n",
      "Freezing vit.encoder.layer.5.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.5.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.5.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.5.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.6.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.6.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.6.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.6.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.6.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.6.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.6.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.6.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.6.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.6.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.6.output.dense.weight\n",
      "Freezing vit.encoder.layer.6.output.dense.bias\n",
      "Freezing vit.encoder.layer.6.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.6.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.6.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.6.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.7.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.7.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.7.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.7.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.7.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.7.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.7.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.7.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.7.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.7.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.7.output.dense.weight\n",
      "Freezing vit.encoder.layer.7.output.dense.bias\n",
      "Freezing vit.encoder.layer.7.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.7.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.7.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.7.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.8.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.8.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.8.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.8.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.8.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.8.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.8.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.8.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.8.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.8.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.8.output.dense.weight\n",
      "Freezing vit.encoder.layer.8.output.dense.bias\n",
      "Freezing vit.encoder.layer.8.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.8.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.8.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.8.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.9.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.9.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.9.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.9.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.9.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.9.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.9.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.9.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.9.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.9.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.9.output.dense.weight\n",
      "Freezing vit.encoder.layer.9.output.dense.bias\n",
      "Freezing vit.encoder.layer.9.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.9.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.9.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.9.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.10.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.10.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.10.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.10.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.10.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.10.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.10.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.10.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.10.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.10.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.10.output.dense.weight\n",
      "Freezing vit.encoder.layer.10.output.dense.bias\n",
      "Freezing vit.encoder.layer.10.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.10.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.10.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.10.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.11.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.11.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.11.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.11.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.11.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.11.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.11.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.11.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.11.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.11.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.11.output.dense.weight\n",
      "Freezing vit.encoder.layer.11.output.dense.bias\n",
      "Freezing vit.encoder.layer.11.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.11.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.11.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.11.layernorm_after.bias\n",
      "Freezing vit.layernorm.weight\n",
      "Freezing vit.layernorm.bias\n",
      "Unfreezing classifier.weight\n",
      "Unfreezing classifier.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr_scheduler_type' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optim' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "701da1ba62a940f09f13666eabab3f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2589, 'grad_norm': 14.474167823791504, 'learning_rate': 1e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d9a6a5b622d47e38b5c37352451e5ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': 1.1251177787780762, 'train_accuracy': 0.3, 'train_f1': 0.13846153846153847, 'train_precision': 0.09, 'train_recall': 0.3, 'train_specificity': 0.6666666666666666, 'train_runtime': 1.8321, 'train_samples_per_second': 5.458, 'train_steps_per_second': 5.458, 'epoch': 0.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bea49e6e5b44cf496869e0913ca8f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0498793125152588, 'eval_accuracy': 0.5, 'eval_f1': 0.3333333333333333, 'eval_precision': 0.25, 'eval_recall': 0.5, 'eval_specificity': 0.6666666666666666, 'eval_runtime': 1.8259, 'eval_samples_per_second': 5.477, 'eval_steps_per_second': 5.477, 'epoch': 0.8}\n",
      "{'loss': 1.0771, 'grad_norm': 7.614552974700928, 'learning_rate': 8.535533905932739e-06, 'epoch': 1.6}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38293dbc3fc04f59b3aad7fb9447b696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': 1.1218191385269165, 'train_accuracy': 0.3, 'train_f1': 0.13846153846153847, 'train_precision': 0.09, 'train_recall': 0.3, 'train_specificity': 0.6666666666666666, 'train_runtime': 1.8102, 'train_samples_per_second': 5.524, 'train_steps_per_second': 5.524, 'epoch': 1.6}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5575cbdf26014456955329abca7be621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.048577904701233, 'eval_accuracy': 0.5, 'eval_f1': 0.3333333333333333, 'eval_precision': 0.25, 'eval_recall': 0.5, 'eval_specificity': 0.6666666666666666, 'eval_runtime': 1.7492, 'eval_samples_per_second': 5.717, 'eval_steps_per_second': 5.717, 'epoch': 1.6}\n",
      "{'loss': 1.0538, 'grad_norm': 10.747528076171875, 'learning_rate': 5e-06, 'epoch': 2.4}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f0aa6750fb4003b3efacf9e24dca50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': 1.1190893650054932, 'train_accuracy': 0.3, 'train_f1': 0.13846153846153847, 'train_precision': 0.09, 'train_recall': 0.3, 'train_specificity': 0.6666666666666666, 'train_runtime': 1.7908, 'train_samples_per_second': 5.584, 'train_steps_per_second': 5.584, 'epoch': 2.4}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2a19b0106d4c57b7a692b84db4d561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0474255084991455, 'eval_accuracy': 0.5, 'eval_f1': 0.3333333333333333, 'eval_precision': 0.25, 'eval_recall': 0.5, 'eval_specificity': 0.6666666666666666, 'eval_runtime': 1.7805, 'eval_samples_per_second': 5.616, 'eval_steps_per_second': 5.616, 'epoch': 2.4}\n",
      "{'loss': 1.0516, 'grad_norm': 6.99785852432251, 'learning_rate': 1.4644660940672628e-06, 'epoch': 3.2}\n",
      "{'loss': 1.1683, 'grad_norm': 10.936563491821289, 'learning_rate': 0.0, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae583f568ee54fb28a7d0d1776394154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': 1.1170228719711304, 'train_accuracy': 0.3, 'train_f1': 0.13846153846153847, 'train_precision': 0.09, 'train_recall': 0.3, 'train_specificity': 0.6666666666666666, 'train_runtime': 1.8638, 'train_samples_per_second': 5.365, 'train_steps_per_second': 5.365, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd4075fa99a14fd2a3f9ad0d33061a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.04659104347229, 'eval_accuracy': 0.5, 'eval_f1': 0.3333333333333333, 'eval_precision': 0.25, 'eval_recall': 0.5, 'eval_specificity': 0.6666666666666666, 'eval_runtime': 1.9041, 'eval_samples_per_second': 5.252, 'eval_steps_per_second': 5.252, 'epoch': 4.0}\n",
      "{'train_runtime': 23.9384, 'train_samples_per_second': 2.089, 'train_steps_per_second': 0.209, 'train_loss': 1.121946358680725, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33f09562a4c4293903784d098c0acd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Fold 2/2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MEGVisionTransformer were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing vit.embeddings.cls_token\n",
      "Freezing vit.embeddings.position_embeddings\n",
      "Freezing vit.embeddings.patch_embeddings.projection.weight\n",
      "Freezing vit.embeddings.patch_embeddings.projection.bias\n",
      "Freezing vit.encoder.layer.0.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.0.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.0.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.0.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.0.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.0.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.0.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.0.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.0.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.0.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.0.output.dense.weight\n",
      "Freezing vit.encoder.layer.0.output.dense.bias\n",
      "Freezing vit.encoder.layer.0.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.0.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.0.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.0.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.1.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.1.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.1.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.1.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.1.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.1.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.1.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.1.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.1.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.1.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.1.output.dense.weight\n",
      "Freezing vit.encoder.layer.1.output.dense.bias\n",
      "Freezing vit.encoder.layer.1.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.1.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.1.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.1.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.2.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.2.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.2.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.2.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.2.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.2.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.2.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.2.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.2.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.2.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.2.output.dense.weight\n",
      "Freezing vit.encoder.layer.2.output.dense.bias\n",
      "Freezing vit.encoder.layer.2.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.2.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.2.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.2.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.3.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.3.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.3.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.3.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.3.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.3.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.3.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.3.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.3.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.3.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.3.output.dense.weight\n",
      "Freezing vit.encoder.layer.3.output.dense.bias\n",
      "Freezing vit.encoder.layer.3.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.3.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.3.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.3.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.4.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.4.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.4.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.4.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.4.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.4.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.4.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.4.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.4.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.4.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.4.output.dense.weight\n",
      "Freezing vit.encoder.layer.4.output.dense.bias\n",
      "Freezing vit.encoder.layer.4.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.4.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.4.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.4.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.5.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.5.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.5.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.5.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.5.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.5.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.5.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.5.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.5.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.5.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.5.output.dense.weight\n",
      "Freezing vit.encoder.layer.5.output.dense.bias\n",
      "Freezing vit.encoder.layer.5.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.5.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.5.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.5.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.6.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.6.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.6.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.6.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.6.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.6.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.6.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.6.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.6.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.6.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.6.output.dense.weight\n",
      "Freezing vit.encoder.layer.6.output.dense.bias\n",
      "Freezing vit.encoder.layer.6.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.6.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.6.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.6.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.7.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.7.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.7.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.7.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.7.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.7.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.7.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.7.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.7.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.7.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.7.output.dense.weight\n",
      "Freezing vit.encoder.layer.7.output.dense.bias\n",
      "Freezing vit.encoder.layer.7.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.7.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.7.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.7.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.8.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.8.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.8.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.8.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.8.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.8.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.8.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.8.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.8.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.8.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.8.output.dense.weight\n",
      "Freezing vit.encoder.layer.8.output.dense.bias\n",
      "Freezing vit.encoder.layer.8.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.8.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.8.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.8.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.9.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.9.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.9.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.9.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.9.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.9.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.9.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.9.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.9.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.9.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.9.output.dense.weight\n",
      "Freezing vit.encoder.layer.9.output.dense.bias\n",
      "Freezing vit.encoder.layer.9.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.9.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.9.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.9.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.10.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.10.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.10.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.10.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.10.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.10.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.10.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.10.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.10.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.10.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.10.output.dense.weight\n",
      "Freezing vit.encoder.layer.10.output.dense.bias\n",
      "Freezing vit.encoder.layer.10.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.10.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.10.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.10.layernorm_after.bias\n",
      "Freezing vit.encoder.layer.11.attention.attention.query.weight\n",
      "Freezing vit.encoder.layer.11.attention.attention.query.bias\n",
      "Freezing vit.encoder.layer.11.attention.attention.key.weight\n",
      "Freezing vit.encoder.layer.11.attention.attention.key.bias\n",
      "Freezing vit.encoder.layer.11.attention.attention.value.weight\n",
      "Freezing vit.encoder.layer.11.attention.attention.value.bias\n",
      "Freezing vit.encoder.layer.11.attention.output.dense.weight\n",
      "Freezing vit.encoder.layer.11.attention.output.dense.bias\n",
      "Freezing vit.encoder.layer.11.intermediate.dense.weight\n",
      "Freezing vit.encoder.layer.11.intermediate.dense.bias\n",
      "Freezing vit.encoder.layer.11.output.dense.weight\n",
      "Freezing vit.encoder.layer.11.output.dense.bias\n",
      "Freezing vit.encoder.layer.11.layernorm_before.weight\n",
      "Freezing vit.encoder.layer.11.layernorm_before.bias\n",
      "Freezing vit.encoder.layer.11.layernorm_after.weight\n",
      "Freezing vit.encoder.layer.11.layernorm_after.bias\n",
      "Freezing vit.layernorm.weight\n",
      "Freezing vit.layernorm.bias\n",
      "Unfreezing classifier.weight\n",
      "Unfreezing classifier.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr_scheduler_type' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optim' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6c61d18b4a494793655d12660f4f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8922, 'grad_norm': 3.3664658069610596, 'learning_rate': 1e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16055d2d9dea48f3ba54b51ff9292020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': 1.0877230167388916, 'train_accuracy': 0.5, 'train_f1': 0.3333333333333333, 'train_precision': 0.25, 'train_recall': 0.5, 'train_specificity': 0.6666666666666666, 'train_runtime': 1.7928, 'train_samples_per_second': 5.578, 'train_steps_per_second': 5.578, 'epoch': 0.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e669609bf2df43e8bd08eb023b1159b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3348338603973389, 'eval_accuracy': 0.3, 'eval_f1': 0.13846153846153847, 'eval_precision': 0.09, 'eval_recall': 0.3, 'eval_specificity': 0.6666666666666666, 'eval_runtime': 1.8239, 'eval_samples_per_second': 5.483, 'eval_steps_per_second': 5.483, 'epoch': 0.8}\n",
      "{'loss': 1.2481, 'grad_norm': 10.208475112915039, 'learning_rate': 8.535533905932739e-06, 'epoch': 1.6}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6812db38af45b6a7b9d9509d7a8c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': 1.085632562637329, 'train_accuracy': 0.5, 'train_f1': 0.3333333333333333, 'train_precision': 0.25, 'train_recall': 0.5, 'train_specificity': 0.6666666666666666, 'train_runtime': 1.8371, 'train_samples_per_second': 5.443, 'train_steps_per_second': 5.443, 'epoch': 1.6}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e52277cf37f41c3ac1c9843ee86e199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.331908106803894, 'eval_accuracy': 0.3, 'eval_f1': 0.13846153846153847, 'eval_precision': 0.09, 'eval_recall': 0.3, 'eval_specificity': 0.6666666666666666, 'eval_runtime': 1.7884, 'eval_samples_per_second': 5.591, 'eval_steps_per_second': 5.591, 'epoch': 1.6}\n",
      "{'loss': 0.9878, 'grad_norm': 7.806415557861328, 'learning_rate': 5e-06, 'epoch': 2.4}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d5d9d2460a4857956723ec4114bfff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': 1.083958625793457, 'train_accuracy': 0.5, 'train_f1': 0.3333333333333333, 'train_precision': 0.25, 'train_recall': 0.5, 'train_specificity': 0.6666666666666666, 'train_runtime': 1.7853, 'train_samples_per_second': 5.601, 'train_steps_per_second': 5.601, 'epoch': 2.4}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90cd9510e62a41768d626e6253bced4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.329612374305725, 'eval_accuracy': 0.3, 'eval_f1': 0.13846153846153847, 'eval_precision': 0.09, 'eval_recall': 0.3, 'eval_specificity': 0.6666666666666666, 'eval_runtime': 1.8014, 'eval_samples_per_second': 5.551, 'eval_steps_per_second': 5.551, 'epoch': 2.4}\n",
      "{'loss': 1.2316, 'grad_norm': 10.576590538024902, 'learning_rate': 1.4644660940672628e-06, 'epoch': 3.2}\n",
      "{'loss': 1.0678, 'grad_norm': 6.151747703552246, 'learning_rate': 0.0, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b8580985ee43018eecbd1528a0d83b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': 1.082637906074524, 'train_accuracy': 0.5, 'train_f1': 0.3333333333333333, 'train_precision': 0.25, 'train_recall': 0.5, 'train_specificity': 0.6666666666666666, 'train_runtime': 1.7905, 'train_samples_per_second': 5.585, 'train_steps_per_second': 5.585, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1fa1c2960f42999c426926a935000f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3276686668395996, 'eval_accuracy': 0.3, 'eval_f1': 0.13846153846153847, 'eval_precision': 0.09, 'eval_recall': 0.3, 'eval_specificity': 0.6666666666666666, 'eval_runtime': 1.7939, 'eval_samples_per_second': 5.574, 'eval_steps_per_second': 5.574, 'epoch': 4.0}\n",
      "{'train_runtime': 23.1728, 'train_samples_per_second': 2.158, 'train_steps_per_second': 0.216, 'train_loss': 1.085502803325653, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb4d115a1234afca33e5e10e9932cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td></td></tr><tr><td>avg_eval_loss</td><td></td></tr><tr><td>eval/accuracy</td><td></td></tr><tr><td>eval/f1</td><td></td></tr><tr><td>eval/loss</td><td></td></tr><tr><td>eval/precision</td><td></td></tr><tr><td>eval/recall</td><td></td></tr><tr><td>eval/runtime</td><td></td></tr><tr><td>eval/samples_per_second</td><td></td></tr><tr><td>eval/specificity</td><td></td></tr><tr><td>eval/steps_per_second</td><td></td></tr><tr><td>f1</td><td></td></tr><tr><td>fold_1/epoch</td><td></td></tr><tr><td>fold_1/eval_accuracy</td><td></td></tr><tr><td>fold_1/eval_f1</td><td></td></tr><tr><td>fold_1/eval_loss</td><td></td></tr><tr><td>fold_1/eval_precision</td><td></td></tr><tr><td>fold_1/eval_recall</td><td></td></tr><tr><td>fold_1/eval_runtime</td><td></td></tr><tr><td>fold_1/eval_samples_per_second</td><td></td></tr><tr><td>fold_1/eval_specificity</td><td></td></tr><tr><td>fold_1/eval_steps_per_second</td><td></td></tr><tr><td>fold_1/grad_norm</td><td></td></tr><tr><td>fold_1/learning_rate</td><td></td></tr><tr><td>fold_1/loss</td><td></td></tr><tr><td>fold_1/total_flos</td><td></td></tr><tr><td>fold_1/train_accuracy</td><td></td></tr><tr><td>fold_1/train_f1</td><td></td></tr><tr><td>fold_1/train_loss</td><td></td></tr><tr><td>fold_1/train_precision</td><td></td></tr><tr><td>fold_1/train_recall</td><td></td></tr><tr><td>fold_1/train_runtime</td><td></td></tr><tr><td>fold_1/train_samples_per_second</td><td></td></tr><tr><td>fold_1/train_specificity</td><td></td></tr><tr><td>fold_1/train_steps_per_second</td><td></td></tr><tr><td>fold_1_eval_loss</td><td></td></tr><tr><td>fold_2/epoch</td><td></td></tr><tr><td>fold_2/eval_accuracy</td><td></td></tr><tr><td>fold_2/eval_f1</td><td></td></tr><tr><td>fold_2/eval_loss</td><td></td></tr><tr><td>fold_2/eval_precision</td><td></td></tr><tr><td>fold_2/eval_recall</td><td></td></tr><tr><td>fold_2/eval_runtime</td><td></td></tr><tr><td>fold_2/eval_samples_per_second</td><td></td></tr><tr><td>fold_2/eval_specificity</td><td></td></tr><tr><td>fold_2/eval_steps_per_second</td><td></td></tr><tr><td>fold_2/grad_norm</td><td></td></tr><tr><td>fold_2/learning_rate</td><td></td></tr><tr><td>fold_2/loss</td><td></td></tr><tr><td>fold_2/total_flos</td><td></td></tr><tr><td>fold_2/train_accuracy</td><td></td></tr><tr><td>fold_2/train_f1</td><td></td></tr><tr><td>fold_2/train_loss</td><td></td></tr><tr><td>fold_2/train_precision</td><td></td></tr><tr><td>fold_2/train_recall</td><td></td></tr><tr><td>fold_2/train_runtime</td><td></td></tr><tr><td>fold_2/train_samples_per_second</td><td></td></tr><tr><td>fold_2/train_specificity</td><td></td></tr><tr><td>fold_2/train_steps_per_second</td><td></td></tr><tr><td>fold_2_eval_loss</td><td></td></tr><tr><td>precision</td><td></td></tr><tr><td>recall</td><td></td></tr><tr><td>specificity</td><td></td></tr><tr><td>train/epoch</td><td></td></tr><tr><td>train/global_step</td><td></td></tr><tr><td>train/grad_norm</td><td></td></tr><tr><td>train/learning_rate</td><td></td></tr><tr><td>train/loss</td><td></td></tr><tr><td>train/train_accuracy</td><td></td></tr><tr><td>train/train_f1</td><td></td></tr><tr><td>train/train_precision</td><td></td></tr><tr><td>train/train_recall</td><td></td></tr><tr><td>train/train_specificity</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.3</td></tr><tr><td>avg_eval_loss</td><td>1.18713</td></tr><tr><td>eval/accuracy</td><td>0.3</td></tr><tr><td>eval/f1</td><td>0.13846</td></tr><tr><td>eval/loss</td><td>1.32767</td></tr><tr><td>eval/precision</td><td>0.09</td></tr><tr><td>eval/recall</td><td>0.3</td></tr><tr><td>eval/runtime</td><td>1.8347</td></tr><tr><td>eval/samples_per_second</td><td>5.45</td></tr><tr><td>eval/specificity</td><td>0.66667</td></tr><tr><td>eval/steps_per_second</td><td>5.45</td></tr><tr><td>f1</td><td>0.13846</td></tr><tr><td>fold_1/epoch</td><td>4</td></tr><tr><td>fold_1/eval_accuracy</td><td>0.5</td></tr><tr><td>fold_1/eval_f1</td><td>0.33333</td></tr><tr><td>fold_1/eval_loss</td><td>1.04659</td></tr><tr><td>fold_1/eval_precision</td><td>0.25</td></tr><tr><td>fold_1/eval_recall</td><td>0.5</td></tr><tr><td>fold_1/eval_runtime</td><td>1.9402</td></tr><tr><td>fold_1/eval_samples_per_second</td><td>5.154</td></tr><tr><td>fold_1/eval_specificity</td><td>0.66667</td></tr><tr><td>fold_1/eval_steps_per_second</td><td>5.154</td></tr><tr><td>fold_1/grad_norm</td><td>10.93656</td></tr><tr><td>fold_1/learning_rate</td><td>0</td></tr><tr><td>fold_1/loss</td><td>1.1683</td></tr><tr><td>fold_1/total_flos</td><td>9109344096092160.0</td></tr><tr><td>fold_1/train_accuracy</td><td>0.3</td></tr><tr><td>fold_1/train_f1</td><td>0.13846</td></tr><tr><td>fold_1/train_loss</td><td>1.12195</td></tr><tr><td>fold_1/train_precision</td><td>0.09</td></tr><tr><td>fold_1/train_recall</td><td>0.3</td></tr><tr><td>fold_1/train_runtime</td><td>23.9384</td></tr><tr><td>fold_1/train_samples_per_second</td><td>2.089</td></tr><tr><td>fold_1/train_specificity</td><td>0.66667</td></tr><tr><td>fold_1/train_steps_per_second</td><td>0.209</td></tr><tr><td>fold_1_eval_loss</td><td>1.04659</td></tr><tr><td>fold_2/epoch</td><td>4</td></tr><tr><td>fold_2/eval_accuracy</td><td>0.3</td></tr><tr><td>fold_2/eval_f1</td><td>0.13846</td></tr><tr><td>fold_2/eval_loss</td><td>1.32767</td></tr><tr><td>fold_2/eval_precision</td><td>0.09</td></tr><tr><td>fold_2/eval_recall</td><td>0.3</td></tr><tr><td>fold_2/eval_runtime</td><td>1.8347</td></tr><tr><td>fold_2/eval_samples_per_second</td><td>5.45</td></tr><tr><td>fold_2/eval_specificity</td><td>0.66667</td></tr><tr><td>fold_2/eval_steps_per_second</td><td>5.45</td></tr><tr><td>fold_2/grad_norm</td><td>6.15175</td></tr><tr><td>fold_2/learning_rate</td><td>0</td></tr><tr><td>fold_2/loss</td><td>1.0678</td></tr><tr><td>fold_2/total_flos</td><td>9109344096092160.0</td></tr><tr><td>fold_2/train_accuracy</td><td>0.5</td></tr><tr><td>fold_2/train_f1</td><td>0.33333</td></tr><tr><td>fold_2/train_loss</td><td>1.0855</td></tr><tr><td>fold_2/train_precision</td><td>0.25</td></tr><tr><td>fold_2/train_recall</td><td>0.5</td></tr><tr><td>fold_2/train_runtime</td><td>23.1728</td></tr><tr><td>fold_2/train_samples_per_second</td><td>2.158</td></tr><tr><td>fold_2/train_specificity</td><td>0.66667</td></tr><tr><td>fold_2/train_steps_per_second</td><td>0.216</td></tr><tr><td>fold_2_eval_loss</td><td>1.32767</td></tr><tr><td>precision</td><td>0.09</td></tr><tr><td>recall</td><td>0.3</td></tr><tr><td>specificity</td><td>0.66667</td></tr><tr><td>total_flos</td><td>9109344096092160.0</td></tr><tr><td>train/epoch</td><td>4</td></tr><tr><td>train/global_step</td><td>5</td></tr><tr><td>train/grad_norm</td><td>6.15175</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>1.0678</td></tr><tr><td>train/train_accuracy</td><td>0.5</td></tr><tr><td>train/train_f1</td><td>0.33333</td></tr><tr><td>train/train_precision</td><td>0.25</td></tr><tr><td>train/train_recall</td><td>0.5</td></tr><tr><td>train/train_specificity</td><td>0.66667</td></tr><tr><td>train_loss</td><td>1.0855</td></tr><tr><td>train_runtime</td><td>23.1728</td></tr><tr><td>train_samples_per_second</td><td>2.158</td></tr><tr><td>train_steps_per_second</td><td>0.216</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ViT_lr:1e-05_optim:adamw_torch_sched:cosine_grads:8</strong> at: <a href='https://wandb.ai/megdecoding/VIT-KFold-HyperSweep/runs/m6rikv8v' target=\"_blank\">https://wandb.ai/megdecoding/VIT-KFold-HyperSweep/runs/m6rikv8v</a><br> View project at: <a href='https://wandb.ai/megdecoding/VIT-KFold-HyperSweep' target=\"_blank\">https://wandb.ai/megdecoding/VIT-KFold-HyperSweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250227_190253-m6rikv8v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"VIT-KFold-HyperSweep\")\n",
    "wandb.agent(sweep_id, \n",
    "            lambda: cross_validate_kfold(train_dataset=dataset['train'], \n",
    "                                         model_class=MEGVisionTransformer, \n",
    "                                         model_name=model_checkpoint,\n",
    "                                         num_classes=3, \n",
    "                                         freeze_type=\"classifier\", \n",
    "                                         k=2), \n",
    "            count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
