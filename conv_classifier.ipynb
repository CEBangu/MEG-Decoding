{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2x Better than Chance within trial but does not generalize well accross trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import models, transforms\n",
    "import torch.nn.functional as F\n",
    "from BcomMEG import *\n",
    "from MEGDataset_Conv import *\n",
    "from ConvNet import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So one option would be to use ResNet as the pretrained and try to fine-tune it. \n",
    "# I'm not opposed to this, but it would require messing with the first layer, and\n",
    "# making sort of arbitrary downscaling of the MEG data. We would have to be clever\n",
    "# about this. I think I'm just going to try to train my own for right now and see\n",
    "# how it goes\n",
    "# resnet = models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
    "# print(resnet.conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/Volumes/@neurospeech/PROJECTS/BCI/BCOM/DATA_ANALYZED/EVOKED/DATA/WITHOUT_BADS/COVERT'\n",
    "dir1 = '/Users/ciprianbangu/Cogmaster/M2 Internship/BCI code/Data_Sample'\n",
    "subjects = ['BCOM_18_2']\n",
    "picks = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = BcomMEG(dir=dir, subjects=subjects, picks=picks, avoid_reading=True)\n",
    "data.upscale(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MEGDataset_Conv(data, label_map='multi_class_covert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "  Data shape: torch.Size([10, 1, 247, 241])\n",
      "  Labels shape: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "train_indices, test_indices = train_test_split(\n",
    "    list(range(len(dataset))), \n",
    "    test_size=0.4, \n",
    "    random_state=42, \n",
    "    stratify=dataset.labels)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "for batch_index, (data, labels) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_index}:\")\n",
    "    print(f\"  Data shape: {data.shape}\")\n",
    "    print(f\"  Labels shape: {labels.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 50], Step [10/13], Loss: 10.0650\n",
      "Epoch [2 / 50], Step [10/13], Loss: 6.3177\n",
      "Epoch [3 / 50], Step [10/13], Loss: 2.6778\n",
      "Epoch [4 / 50], Step [10/13], Loss: 0.5450\n",
      "Epoch [5 / 50], Step [10/13], Loss: 0.0068\n",
      "Epoch [6 / 50], Step [10/13], Loss: 0.0035\n",
      "Epoch [7 / 50], Step [10/13], Loss: 0.0020\n",
      "Epoch [8 / 50], Step [10/13], Loss: 0.0004\n",
      "Epoch [9 / 50], Step [10/13], Loss: 0.0003\n",
      "Epoch [10 / 50], Step [10/13], Loss: 0.0001\n",
      "Epoch [11 / 50], Step [10/13], Loss: 0.0001\n",
      "Epoch [12 / 50], Step [10/13], Loss: 0.0001\n",
      "Epoch [13 / 50], Step [10/13], Loss: 0.0004\n",
      "Epoch [14 / 50], Step [10/13], Loss: 0.0003\n",
      "Epoch [15 / 50], Step [10/13], Loss: 0.0002\n",
      "Epoch [16 / 50], Step [10/13], Loss: 0.0001\n",
      "Epoch [17 / 50], Step [10/13], Loss: 0.0001\n",
      "Epoch [18 / 50], Step [10/13], Loss: 0.0003\n",
      "Epoch [19 / 50], Step [10/13], Loss: 0.0002\n",
      "Epoch [20 / 50], Step [10/13], Loss: 0.0004\n",
      "Epoch [21 / 50], Step [10/13], Loss: 0.0003\n",
      "Epoch [22 / 50], Step [10/13], Loss: 0.0001\n",
      "Epoch [23 / 50], Step [10/13], Loss: 0.0002\n",
      "Epoch [24 / 50], Step [10/13], Loss: 0.0001\n",
      "Epoch [25 / 50], Step [10/13], Loss: 0.0001\n",
      "Epoch [26 / 50], Step [10/13], Loss: 0.0001\n",
      "Epoch [27 / 50], Step [10/13], Loss: 0.0002\n",
      "Epoch [28 / 50], Step [10/13], Loss: 0.0002\n",
      "Epoch [29 / 50], Step [10/13], Loss: 0.0001\n",
      "Epoch [30 / 50], Step [10/13], Loss: 0.0002\n",
      "Epoch [31 / 50], Step [10/13], Loss: 0.0000\n",
      "Epoch [32 / 50], Step [10/13], Loss: 0.0001\n",
      "Epoch [33 / 50], Step [10/13], Loss: 0.0001\n",
      "Epoch [34 / 50], Step [10/13], Loss: 0.0002\n",
      "Epoch [35 / 50], Step [10/13], Loss: 0.0003\n",
      "Epoch [36 / 50], Step [10/13], Loss: 0.0000\n",
      "Epoch [37 / 50], Step [10/13], Loss: 0.0004\n",
      "Epoch [38 / 50], Step [10/13], Loss: 0.0001\n",
      "Epoch [39 / 50], Step [10/13], Loss: 0.0002\n",
      "Epoch [40 / 50], Step [10/13], Loss: 0.0003\n",
      "Epoch [41 / 50], Step [10/13], Loss: 0.0001\n",
      "Epoch [42 / 50], Step [10/13], Loss: 0.0002\n",
      "Epoch [43 / 50], Step [10/13], Loss: 0.0000\n",
      "Epoch [44 / 50], Step [10/13], Loss: 0.0001\n",
      "Epoch [45 / 50], Step [10/13], Loss: 0.0001\n",
      "Epoch [46 / 50], Step [10/13], Loss: 0.0001\n",
      "Epoch [47 / 50], Step [10/13], Loss: 0.0001\n",
      "Epoch [48 / 50], Step [10/13], Loss: 0.0002\n",
      "Epoch [49 / 50], Step [10/13], Loss: 0.0001\n",
      "Epoch [50 / 50], Step [10/13], Loss: 0.0001\n",
      "Training Finished\n"
     ]
    }
   ],
   "source": [
    "device = 'mps'\n",
    "model = ConvNet().to(device=device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "epochs = 50\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "\n",
    "loss_values = []\n",
    "for epoch in range(epochs):\n",
    "    for i, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device, dtype=torch.float32), labels.to(device, dtype=torch.long)\n",
    "        # print(data)\n",
    "\n",
    "        outputs = model(data)\n",
    "        # print(f\"Outputs: {outputs}\")\n",
    "        # print(f\"Labels: {labels}\")\n",
    "        loss = criterion(outputs, labels)\n",
    "        # print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_values.append(loss.item())\n",
    "\n",
    "        if (i+1) % 10 == 0:  # Adjusted the condition to a smaller number\n",
    "            print(f'Epoch [{epoch + 1} / {epochs}], Step [{i + 1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "print('Training Finished')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x372ca70b0>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(loss_values)), loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/50], Test Loss: 5.1966, Test Accuracy: 7.2289\n",
      "Test Loss: 5.1966, F1-Score: 0.0601\n"
     ]
    }
   ],
   "source": [
    "test_loss_values = []  # To store test loss\n",
    "test_accuracies = [] \n",
    "all_preds = []\n",
    "all_labels = []\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "    for data, labels in test_loader:\n",
    "        data, labels = data.to(device, dtype=torch.float32), labels.to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)  # Get class with max logit\n",
    "        all_preds.extend(predicted.cpu().numpy())  # Collect predictions\n",
    "        all_labels.extend(labels.cpu().numpy()) \n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "test_loss /= len(test_loader)  # Average test loss\n",
    "accuracy = correct / total  # Test accuracy\n",
    "test_loss_values.append(test_loss)\n",
    "test_accuracies.append(accuracy)\n",
    "\n",
    "print(f'Epoch [{epoch + 1}/{epochs}], Test Loss: {test_loss:.4f}, Test Accuracy: {100 * accuracy:.4f}')\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "print(f\"Test Loss: {test_loss:.4f}, F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "2\n",
      "0\n",
      "7\n",
      "5\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "for i in (np.where(np.array(all_labels) == np.array(all_preds))[0]):\n",
    "    print (all_preds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.555555555555555"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/len(np.unique(dataset.labels)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the obvious extension is to train it on more data. Perhaps train it on first 2 trials and have it predict on the last one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accross trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = BcomMEG(subjects=['BCOM_18_2', 'BCOM_18_3', 'BCOM_18_4'], dir=dir, picks=picks, avoid_reading=True)\n",
    "data2.upscale(13)\n",
    "tensor_2 = MEGDataset_Conv(data2, label_map='multi_class_covert')\n",
    "dataset_2_size = len(tensor_2)\n",
    "indices = list(range(dataset_2_size))\n",
    "train_indices, test_indices = train_test_split(\n",
    "    list(range(len(tensor_2))), \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=tensor_2.labels)\n",
    "\n",
    "train_dataset_2 = Subset(tensor_2, train_indices)\n",
    "test_dataset_2 = Subset(tensor_2, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = [tensor_2.labels[i] for i in test_dataset_2.indices]\n",
    "plt.hist(test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [tensor_2.labels[i] for i in train_dataset_2.indices]\n",
    "plt.hist(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "  Data shape: torch.Size([10, 1, 247, 241])\n",
      "  Labels shape: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "train_loader_2 = DataLoader(train_dataset_2, batch_size=10, shuffle=True)\n",
    "test_loader_2 = DataLoader(test_dataset_2, batch_size=1, shuffle=False)\n",
    "for batch_index, (data, labels) in enumerate(train_loader_2):\n",
    "    print(f\"Batch {batch_index}:\")\n",
    "    print(f\"  Data shape: {data.shape}\")\n",
    "    print(f\"  Labels shape: {labels.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 30], Step [10/49], Loss: 3.9814\n",
      "Epoch [1 / 30], Step [20/49], Loss: 3.5633\n",
      "Epoch [1 / 30], Step [30/49], Loss: 3.5361\n",
      "Epoch [1 / 30], Step [40/49], Loss: 4.6772\n",
      "Epoch [2 / 30], Step [10/49], Loss: 1.8335\n",
      "Epoch [2 / 30], Step [20/49], Loss: 2.3736\n",
      "Epoch [2 / 30], Step [30/49], Loss: 1.8009\n",
      "Epoch [2 / 30], Step [40/49], Loss: 2.1431\n",
      "Epoch [3 / 30], Step [10/49], Loss: 1.7802\n",
      "Epoch [3 / 30], Step [20/49], Loss: 0.6134\n",
      "Epoch [3 / 30], Step [30/49], Loss: 1.2379\n",
      "Epoch [3 / 30], Step [40/49], Loss: 0.6375\n",
      "Epoch [4 / 30], Step [10/49], Loss: 0.1487\n",
      "Epoch [4 / 30], Step [20/49], Loss: 0.2172\n",
      "Epoch [4 / 30], Step [30/49], Loss: 0.2646\n",
      "Epoch [4 / 30], Step [40/49], Loss: 0.1346\n",
      "Epoch [5 / 30], Step [10/49], Loss: 0.1083\n",
      "Epoch [5 / 30], Step [20/49], Loss: 0.0635\n",
      "Epoch [5 / 30], Step [30/49], Loss: 0.0244\n",
      "Epoch [5 / 30], Step [40/49], Loss: 0.0531\n",
      "Epoch [6 / 30], Step [10/49], Loss: 0.0194\n",
      "Epoch [6 / 30], Step [20/49], Loss: 0.0120\n",
      "Epoch [6 / 30], Step [30/49], Loss: 0.0196\n",
      "Epoch [6 / 30], Step [40/49], Loss: 0.0407\n",
      "Epoch [7 / 30], Step [10/49], Loss: 0.0178\n",
      "Epoch [7 / 30], Step [20/49], Loss: 0.0119\n",
      "Epoch [7 / 30], Step [30/49], Loss: 0.0109\n",
      "Epoch [7 / 30], Step [40/49], Loss: 0.0175\n",
      "Epoch [8 / 30], Step [10/49], Loss: 0.0108\n",
      "Epoch [8 / 30], Step [20/49], Loss: 0.0115\n",
      "Epoch [8 / 30], Step [30/49], Loss: 0.0076\n",
      "Epoch [8 / 30], Step [40/49], Loss: 0.0134\n",
      "Epoch [9 / 30], Step [10/49], Loss: 0.0138\n",
      "Epoch [9 / 30], Step [20/49], Loss: 0.0073\n",
      "Epoch [9 / 30], Step [30/49], Loss: 0.0087\n",
      "Epoch [9 / 30], Step [40/49], Loss: 0.0106\n",
      "Epoch [10 / 30], Step [10/49], Loss: 0.0082\n",
      "Epoch [10 / 30], Step [20/49], Loss: 0.0109\n",
      "Epoch [10 / 30], Step [30/49], Loss: 0.0049\n",
      "Epoch [10 / 30], Step [40/49], Loss: 0.0084\n",
      "Epoch [11 / 30], Step [10/49], Loss: 0.0057\n",
      "Epoch [11 / 30], Step [20/49], Loss: 0.0082\n",
      "Epoch [11 / 30], Step [30/49], Loss: 0.0078\n",
      "Epoch [11 / 30], Step [40/49], Loss: 0.0083\n",
      "Epoch [12 / 30], Step [10/49], Loss: 0.0064\n",
      "Epoch [12 / 30], Step [20/49], Loss: 0.0067\n",
      "Epoch [12 / 30], Step [30/49], Loss: 0.0095\n",
      "Epoch [12 / 30], Step [40/49], Loss: 0.0054\n",
      "Epoch [13 / 30], Step [10/49], Loss: 0.0061\n",
      "Epoch [13 / 30], Step [20/49], Loss: 0.0095\n",
      "Epoch [13 / 30], Step [30/49], Loss: 0.0093\n",
      "Epoch [13 / 30], Step [40/49], Loss: 0.0086\n",
      "Epoch [14 / 30], Step [10/49], Loss: 0.0086\n",
      "Epoch [14 / 30], Step [20/49], Loss: 0.0047\n",
      "Epoch [14 / 30], Step [30/49], Loss: 0.0050\n",
      "Epoch [14 / 30], Step [40/49], Loss: 0.0062\n",
      "Epoch [15 / 30], Step [10/49], Loss: 0.0047\n",
      "Epoch [15 / 30], Step [20/49], Loss: 0.0098\n",
      "Epoch [15 / 30], Step [30/49], Loss: 0.0053\n",
      "Epoch [15 / 30], Step [40/49], Loss: 0.0059\n",
      "Epoch [16 / 30], Step [10/49], Loss: 0.0029\n",
      "Epoch [16 / 30], Step [20/49], Loss: 0.0059\n",
      "Epoch [16 / 30], Step [30/49], Loss: 0.0059\n",
      "Epoch [16 / 30], Step [40/49], Loss: 0.0037\n",
      "Epoch [17 / 30], Step [10/49], Loss: 0.0037\n",
      "Epoch [17 / 30], Step [20/49], Loss: 0.0043\n",
      "Epoch [17 / 30], Step [30/49], Loss: 0.0035\n",
      "Epoch [17 / 30], Step [40/49], Loss: 0.0029\n",
      "Epoch [18 / 30], Step [10/49], Loss: 0.0045\n",
      "Epoch [18 / 30], Step [20/49], Loss: 0.0042\n",
      "Epoch [18 / 30], Step [30/49], Loss: 0.0042\n",
      "Epoch [18 / 30], Step [40/49], Loss: 0.0036\n",
      "Epoch [19 / 30], Step [10/49], Loss: 0.0052\n",
      "Epoch [19 / 30], Step [20/49], Loss: 0.0030\n",
      "Epoch [19 / 30], Step [30/49], Loss: 0.0036\n",
      "Epoch [19 / 30], Step [40/49], Loss: 0.0037\n",
      "Epoch [20 / 30], Step [10/49], Loss: 0.0034\n",
      "Epoch [20 / 30], Step [20/49], Loss: 0.0041\n",
      "Epoch [20 / 30], Step [30/49], Loss: 0.0039\n",
      "Epoch [20 / 30], Step [40/49], Loss: 0.0045\n",
      "Epoch [21 / 30], Step [10/49], Loss: 0.0025\n",
      "Epoch [21 / 30], Step [20/49], Loss: 0.0036\n",
      "Epoch [21 / 30], Step [30/49], Loss: 0.0027\n",
      "Epoch [21 / 30], Step [40/49], Loss: 0.0026\n",
      "Epoch [22 / 30], Step [10/49], Loss: 0.0030\n",
      "Epoch [22 / 30], Step [20/49], Loss: 0.0052\n",
      "Epoch [22 / 30], Step [30/49], Loss: 0.0022\n",
      "Epoch [22 / 30], Step [40/49], Loss: 0.0027\n",
      "Epoch [23 / 30], Step [10/49], Loss: 0.0038\n",
      "Epoch [23 / 30], Step [20/49], Loss: 0.0025\n",
      "Epoch [23 / 30], Step [30/49], Loss: 0.0038\n",
      "Epoch [23 / 30], Step [40/49], Loss: 0.0033\n",
      "Epoch [24 / 30], Step [10/49], Loss: 0.0022\n",
      "Epoch [24 / 30], Step [20/49], Loss: 0.0018\n",
      "Epoch [24 / 30], Step [30/49], Loss: 0.0017\n",
      "Epoch [24 / 30], Step [40/49], Loss: 0.0019\n",
      "Epoch [25 / 30], Step [10/49], Loss: 0.0042\n",
      "Epoch [25 / 30], Step [20/49], Loss: 0.0017\n",
      "Epoch [25 / 30], Step [30/49], Loss: 0.0022\n",
      "Epoch [25 / 30], Step [40/49], Loss: 0.0029\n",
      "Epoch [26 / 30], Step [10/49], Loss: 0.0028\n",
      "Epoch [26 / 30], Step [20/49], Loss: 0.0022\n",
      "Epoch [26 / 30], Step [30/49], Loss: 0.0017\n",
      "Epoch [26 / 30], Step [40/49], Loss: 0.0021\n",
      "Epoch [27 / 30], Step [10/49], Loss: 0.0021\n",
      "Epoch [27 / 30], Step [20/49], Loss: 0.0019\n",
      "Epoch [27 / 30], Step [30/49], Loss: 0.0022\n",
      "Epoch [27 / 30], Step [40/49], Loss: 0.0020\n",
      "Epoch [28 / 30], Step [10/49], Loss: 0.0030\n",
      "Epoch [28 / 30], Step [20/49], Loss: 0.0011\n",
      "Epoch [28 / 30], Step [30/49], Loss: 0.0017\n",
      "Epoch [28 / 30], Step [40/49], Loss: 0.0015\n",
      "Epoch [29 / 30], Step [10/49], Loss: 0.0019\n",
      "Epoch [29 / 30], Step [20/49], Loss: 0.0018\n",
      "Epoch [29 / 30], Step [30/49], Loss: 0.0019\n",
      "Epoch [29 / 30], Step [40/49], Loss: 0.0024\n",
      "Epoch [30 / 30], Step [10/49], Loss: 0.0022\n",
      "Epoch [30 / 30], Step [20/49], Loss: 0.0021\n",
      "Epoch [30 / 30], Step [30/49], Loss: 0.0016\n",
      "Epoch [30 / 30], Step [40/49], Loss: 0.0029\n",
      "Training Finished\n"
     ]
    }
   ],
   "source": [
    "device = 'mps'\n",
    "model_2 = ConvNet().to(device=device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_2.parameters(), lr=0.00005)\n",
    "epochs = 30\n",
    "\n",
    "n_total_steps = len(train_loader_2)\n",
    "\n",
    "loss_values = []\n",
    "for epoch in range(epochs):\n",
    "    for i, (data, labels) in enumerate(train_loader_2):\n",
    "        model_2.train()\n",
    "        data, labels = data.to(device, dtype=torch.float32), labels.to(device,dtype=torch.long)\n",
    "        # print(data)\n",
    "\n",
    "        outputs = model_2(data)\n",
    "        # print(f\"Outputs: {outputs}\")\n",
    "        # print(f\"Labels: {labels}\")\n",
    "        loss = criterion(outputs, labels)\n",
    "        # print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_values.append(loss.item())\n",
    "\n",
    "        if (i+1) % 10 == 0:  # Adjusted the condition to a smaller number\n",
    "            print(f'Epoch [{epoch + 1} / {epochs}], Step [{i + 1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "print('Training Finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 4.0537, Accuracy: 8.26%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[0 1 0 1 1 3 0 0 0 0 0 0 0 0 1 0 0 1]\n",
      " [0 0 0 0 0 1 0 1 0 1 0 1 0 0 2 0 0 0]\n",
      " [0 0 0 0 0 2 0 1 1 0 0 0 0 0 1 1 0 1]\n",
      " [0 0 0 3 0 0 2 2 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 2 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0]\n",
      " [0 0 0 1 0 1 0 0 1 1 2 0 0 1 1 1 0 0]\n",
      " [0 0 2 1 0 0 1 2 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 1 0 0 0 0 0 0 1 1 1 0 1 0 1 0 2 0]\n",
      " [0 0 0 1 0 2 3 0 0 0 1 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0]\n",
      " [0 1 0 1 0 0 1 0 0 3 0 1 0 0 0 0 1 0]\n",
      " [1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0]\n",
      " [1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      " [1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0]\n",
      " [0 0 0 1 0 1 0 1 2 1 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 2 0 1 1 0 0 0 0 0 0 0 0 1 1 0]\n",
      " [0 0 0 2 0 0 2 0 0 1 1 0 0 0 0 0 0 0]]\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.20      0.38      0.26         8\n",
      "           4       0.00      0.00      0.00         3\n",
      "           5       0.07      0.14      0.10         7\n",
      "           6       0.00      0.00      0.00         9\n",
      "           7       0.25      0.29      0.27         7\n",
      "           8       0.14      0.12      0.13         8\n",
      "           9       0.00      0.00      0.00         8\n",
      "          10       0.11      0.17      0.13         6\n",
      "          11       0.25      0.12      0.17         8\n",
      "          12       0.00      0.00      0.00         5\n",
      "          13       0.00      0.00      0.00         5\n",
      "          14       0.00      0.00      0.00         6\n",
      "          15       0.00      0.00      0.00         7\n",
      "          16       0.14      0.14      0.14         7\n",
      "          17       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.08       121\n",
      "   macro avg       0.06      0.08      0.07       121\n",
      "weighted avg       0.07      0.08      0.07       121\n",
      "\n",
      "\n",
      "Class distribution in test set:\n",
      "Counter({6: 9, 9: 8, 11: 8, 3: 8, 0: 8, 8: 8, 5: 7, 7: 7, 2: 7, 15: 7, 16: 7, 14: 6, 17: 6, 10: 6, 1: 6, 13: 5, 12: 5, 4: 3})\n",
      "\n",
      "Sample predictions vs actual:\n",
      "Predicted: 6, Actual: 5\n",
      "Predicted: 1, Actual: 4\n",
      "Predicted: 7, Actual: 14\n",
      "Predicted: 5, Actual: 9\n",
      "Predicted: 10, Actual: 17\n",
      "Predicted: 10, Actual: 6\n",
      "Predicted: 9, Actual: 11\n",
      "Predicted: 3, Actual: 3\n",
      "Predicted: 2, Actual: 7\n",
      "Predicted: 3, Actual: 0\n"
     ]
    }
   ],
   "source": [
    "test_loss_values = []\n",
    "test_accuracies = [] \n",
    "all_preds = []\n",
    "all_labels = []\n",
    "model_2.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader_2:\n",
    "        # Move both data and labels to device\n",
    "        data, labels = data.to(device, dtype=torch.float32), labels.to(device, dtype=torch.long)\n",
    "        \n",
    "        # Get predictions\n",
    "        outputs = model_2(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Accumulate loss\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # Get predictions\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Save predictions and labels for metrics\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    test_loss /= len(test_loader_2)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.4f}, Accuracy: {100 * accuracy:.2f}%')\n",
    "\n",
    "# Print detailed metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, all_preds))\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds))\n",
    "\n",
    "# Additional diagnostics\n",
    "print(\"\\nClass distribution in test set:\")\n",
    "from collections import Counter\n",
    "print(Counter(all_labels))\n",
    "\n",
    "# Look at some predictions\n",
    "print(\"\\nSample predictions vs actual:\")\n",
    "for i in range(min(10, len(all_preds))):\n",
    "    print(f\"Predicted: {all_preds[i]}, Actual: {all_labels[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch [1/10], Train Loss: 5.1316, Train Acc: 6.03%, Val Loss: 5.0003, Val Acc: 7.44%\n",
      "Epoch [2/10], Train Loss: 1.5814, Train Acc: 57.38%, Val Loss: 4.8427, Val Acc: 4.96%\n",
      "Epoch [3/10], Train Loss: 0.4294, Train Acc: 90.44%, Val Loss: 4.3160, Val Acc: 4.13%\n",
      "Epoch [4/10], Train Loss: 0.2049, Train Acc: 96.67%, Val Loss: 4.4251, Val Acc: 2.48%\n",
      "Epoch [5/10], Train Loss: 0.0441, Train Acc: 99.79%, Val Loss: 4.1283, Val Acc: 7.44%\n",
      "Epoch [6/10], Train Loss: 0.0214, Train Acc: 100.00%, Val Loss: 4.0952, Val Acc: 5.79%\n",
      "Epoch [7/10], Train Loss: 0.0156, Train Acc: 100.00%, Val Loss: 4.0970, Val Acc: 6.61%\n",
      "Epoch [8/10], Train Loss: 0.0121, Train Acc: 100.00%, Val Loss: 4.1415, Val Acc: 7.44%\n",
      "Epoch [9/10], Train Loss: 0.0096, Train Acc: 100.00%, Val Loss: 4.1063, Val Acc: 9.09%\n",
      "Epoch [10/10], Train Loss: 0.0085, Train Acc: 100.00%, Val Loss: 4.1794, Val Acc: 9.09%\n",
      "Epoch [11/10], Train Loss: 0.0071, Train Acc: 100.00%, Val Loss: 4.2007, Val Acc: 4.96%\n",
      "Epoch [12/10], Train Loss: 0.0066, Train Acc: 100.00%, Val Loss: 4.1252, Val Acc: 7.44%\n",
      "Epoch [13/10], Train Loss: 0.0058, Train Acc: 100.00%, Val Loss: 4.2264, Val Acc: 8.26%\n",
      "Epoch [14/10], Train Loss: 0.0050, Train Acc: 100.00%, Val Loss: 4.1807, Val Acc: 8.26%\n",
      "Epoch [15/10], Train Loss: 0.0045, Train Acc: 100.00%, Val Loss: 4.2135, Val Acc: 7.44%\n",
      "Epoch [16/10], Train Loss: 0.0041, Train Acc: 100.00%, Val Loss: 4.1280, Val Acc: 8.26%\n",
      "Epoch [17/10], Train Loss: 0.0039, Train Acc: 100.00%, Val Loss: 4.2360, Val Acc: 9.09%\n",
      "Epoch [18/10], Train Loss: 0.0035, Train Acc: 100.00%, Val Loss: 4.2633, Val Acc: 7.44%\n",
      "Epoch [19/10], Train Loss: 0.0032, Train Acc: 100.00%, Val Loss: 4.2115, Val Acc: 7.44%\n",
      "Epoch [20/10], Train Loss: 0.0028, Train Acc: 100.00%, Val Loss: 4.2878, Val Acc: 7.44%\n",
      "Fold 2/5\n",
      "Epoch [1/10], Train Loss: 4.7928, Train Acc: 6.03%, Val Loss: 4.8935, Val Acc: 5.79%\n",
      "Epoch [2/10], Train Loss: 1.5479, Train Acc: 57.80%, Val Loss: 4.5193, Val Acc: 7.44%\n",
      "Epoch [3/10], Train Loss: 0.5089, Train Acc: 86.07%, Val Loss: 4.7809, Val Acc: 5.79%\n",
      "Epoch [4/10], Train Loss: 0.0946, Train Acc: 99.38%, Val Loss: 3.7710, Val Acc: 3.31%\n",
      "Epoch [5/10], Train Loss: 0.0327, Train Acc: 100.00%, Val Loss: 3.8435, Val Acc: 4.96%\n",
      "Epoch [6/10], Train Loss: 0.0184, Train Acc: 100.00%, Val Loss: 3.7920, Val Acc: 7.44%\n",
      "Epoch [7/10], Train Loss: 0.0149, Train Acc: 100.00%, Val Loss: 3.8150, Val Acc: 4.13%\n",
      "Epoch [8/10], Train Loss: 0.0119, Train Acc: 100.00%, Val Loss: 3.8129, Val Acc: 6.61%\n",
      "Epoch [9/10], Train Loss: 0.0098, Train Acc: 100.00%, Val Loss: 3.8114, Val Acc: 4.96%\n",
      "Epoch [10/10], Train Loss: 0.0086, Train Acc: 100.00%, Val Loss: 3.7751, Val Acc: 4.96%\n",
      "Epoch [11/10], Train Loss: 0.0075, Train Acc: 100.00%, Val Loss: 3.8887, Val Acc: 3.31%\n",
      "Epoch [12/10], Train Loss: 0.0067, Train Acc: 100.00%, Val Loss: 3.8325, Val Acc: 4.96%\n",
      "Epoch [13/10], Train Loss: 0.0059, Train Acc: 100.00%, Val Loss: 3.8519, Val Acc: 5.79%\n",
      "Epoch [14/10], Train Loss: 0.0051, Train Acc: 100.00%, Val Loss: 3.8811, Val Acc: 4.96%\n",
      "Epoch [15/10], Train Loss: 0.0046, Train Acc: 100.00%, Val Loss: 3.8975, Val Acc: 4.96%\n",
      "Epoch [16/10], Train Loss: 0.0042, Train Acc: 100.00%, Val Loss: 3.9462, Val Acc: 4.13%\n",
      "Epoch [17/10], Train Loss: 0.0038, Train Acc: 100.00%, Val Loss: 3.8999, Val Acc: 4.96%\n",
      "Epoch [18/10], Train Loss: 0.0035, Train Acc: 100.00%, Val Loss: 3.9376, Val Acc: 4.96%\n",
      "Epoch [19/10], Train Loss: 0.0031, Train Acc: 100.00%, Val Loss: 3.9573, Val Acc: 5.79%\n",
      "Epoch [20/10], Train Loss: 0.0029, Train Acc: 100.00%, Val Loss: 3.9896, Val Acc: 4.96%\n",
      "Fold 3/5\n",
      "Epoch [1/10], Train Loss: 4.9431, Train Acc: 4.36%, Val Loss: 5.4893, Val Acc: 6.67%\n",
      "Epoch [2/10], Train Loss: 1.7739, Train Acc: 50.41%, Val Loss: 5.3143, Val Acc: 5.00%\n",
      "Epoch [3/10], Train Loss: 0.6105, Train Acc: 84.02%, Val Loss: 4.4139, Val Acc: 5.83%\n",
      "Epoch [4/10], Train Loss: 0.1162, Train Acc: 98.76%, Val Loss: 4.1601, Val Acc: 6.67%\n",
      "Epoch [5/10], Train Loss: 0.0454, Train Acc: 99.79%, Val Loss: 3.9879, Val Acc: 6.67%\n",
      "Epoch [6/10], Train Loss: 0.0179, Train Acc: 100.00%, Val Loss: 3.9272, Val Acc: 5.83%\n",
      "Epoch [7/10], Train Loss: 0.0132, Train Acc: 100.00%, Val Loss: 3.8405, Val Acc: 5.00%\n",
      "Epoch [8/10], Train Loss: 0.0109, Train Acc: 100.00%, Val Loss: 3.8644, Val Acc: 6.67%\n",
      "Epoch [9/10], Train Loss: 0.0092, Train Acc: 100.00%, Val Loss: 3.9368, Val Acc: 5.83%\n",
      "Epoch [10/10], Train Loss: 0.0081, Train Acc: 100.00%, Val Loss: 3.9250, Val Acc: 5.83%\n",
      "Epoch [11/10], Train Loss: 0.0068, Train Acc: 100.00%, Val Loss: 3.9377, Val Acc: 6.67%\n",
      "Epoch [12/10], Train Loss: 0.0062, Train Acc: 100.00%, Val Loss: 4.0112, Val Acc: 6.67%\n",
      "Epoch [13/10], Train Loss: 0.0055, Train Acc: 100.00%, Val Loss: 3.9862, Val Acc: 6.67%\n",
      "Epoch [14/10], Train Loss: 0.0049, Train Acc: 100.00%, Val Loss: 3.9928, Val Acc: 5.00%\n",
      "Epoch [15/10], Train Loss: 0.0045, Train Acc: 100.00%, Val Loss: 4.0108, Val Acc: 5.00%\n",
      "Epoch [16/10], Train Loss: 0.0040, Train Acc: 100.00%, Val Loss: 4.0481, Val Acc: 5.00%\n",
      "Epoch [17/10], Train Loss: 0.0036, Train Acc: 100.00%, Val Loss: 4.0095, Val Acc: 5.83%\n",
      "Epoch [18/10], Train Loss: 0.0033, Train Acc: 100.00%, Val Loss: 4.0088, Val Acc: 5.00%\n",
      "Epoch [19/10], Train Loss: 0.0030, Train Acc: 100.00%, Val Loss: 3.9926, Val Acc: 5.00%\n",
      "Epoch [20/10], Train Loss: 0.0028, Train Acc: 100.00%, Val Loss: 4.0642, Val Acc: 5.00%\n",
      "Fold 4/5\n",
      "Epoch [1/10], Train Loss: 5.2597, Train Acc: 4.77%, Val Loss: 5.5565, Val Acc: 7.50%\n",
      "Epoch [2/10], Train Loss: 1.6601, Train Acc: 52.49%, Val Loss: 4.5341, Val Acc: 6.67%\n",
      "Epoch [3/10], Train Loss: 0.4131, Train Acc: 89.21%, Val Loss: 4.1170, Val Acc: 10.00%\n",
      "Epoch [4/10], Train Loss: 0.0974, Train Acc: 99.59%, Val Loss: 3.8925, Val Acc: 5.00%\n",
      "Epoch [5/10], Train Loss: 0.0310, Train Acc: 100.00%, Val Loss: 3.9103, Val Acc: 5.83%\n",
      "Epoch [6/10], Train Loss: 0.0213, Train Acc: 100.00%, Val Loss: 3.8687, Val Acc: 6.67%\n",
      "Epoch [7/10], Train Loss: 0.0164, Train Acc: 100.00%, Val Loss: 3.9175, Val Acc: 4.17%\n",
      "Epoch [8/10], Train Loss: 0.0132, Train Acc: 100.00%, Val Loss: 4.0140, Val Acc: 5.00%\n",
      "Epoch [9/10], Train Loss: 0.0110, Train Acc: 100.00%, Val Loss: 3.9317, Val Acc: 5.00%\n",
      "Epoch [10/10], Train Loss: 0.0096, Train Acc: 100.00%, Val Loss: 3.9022, Val Acc: 4.17%\n",
      "Epoch [11/10], Train Loss: 0.0084, Train Acc: 100.00%, Val Loss: 4.0108, Val Acc: 5.00%\n",
      "Epoch [12/10], Train Loss: 0.0073, Train Acc: 100.00%, Val Loss: 4.0273, Val Acc: 4.17%\n",
      "Epoch [13/10], Train Loss: 0.0062, Train Acc: 100.00%, Val Loss: 4.0109, Val Acc: 3.33%\n",
      "Epoch [14/10], Train Loss: 0.0057, Train Acc: 100.00%, Val Loss: 4.0406, Val Acc: 4.17%\n",
      "Epoch [15/10], Train Loss: 0.0051, Train Acc: 100.00%, Val Loss: 4.0953, Val Acc: 3.33%\n",
      "Epoch [16/10], Train Loss: 0.0045, Train Acc: 100.00%, Val Loss: 4.0026, Val Acc: 4.17%\n",
      "Epoch [17/10], Train Loss: 0.0041, Train Acc: 100.00%, Val Loss: 4.0545, Val Acc: 3.33%\n",
      "Epoch [18/10], Train Loss: 0.0037, Train Acc: 100.00%, Val Loss: 4.1825, Val Acc: 5.83%\n",
      "Epoch [19/10], Train Loss: 0.0033, Train Acc: 100.00%, Val Loss: 4.2020, Val Acc: 4.17%\n",
      "Epoch [20/10], Train Loss: 0.0031, Train Acc: 100.00%, Val Loss: 4.1534, Val Acc: 5.00%\n",
      "Fold 5/5\n",
      "Epoch [1/10], Train Loss: 5.0264, Train Acc: 6.02%, Val Loss: 5.2439, Val Acc: 5.83%\n",
      "Epoch [2/10], Train Loss: 1.4648, Train Acc: 61.62%, Val Loss: 4.8068, Val Acc: 7.50%\n",
      "Epoch [3/10], Train Loss: 0.4808, Train Acc: 86.93%, Val Loss: 3.9806, Val Acc: 7.50%\n",
      "Epoch [4/10], Train Loss: 0.1415, Train Acc: 97.93%, Val Loss: 4.0319, Val Acc: 6.67%\n",
      "Epoch [5/10], Train Loss: 0.0376, Train Acc: 100.00%, Val Loss: 3.8143, Val Acc: 6.67%\n",
      "Epoch [6/10], Train Loss: 0.0171, Train Acc: 100.00%, Val Loss: 3.8205, Val Acc: 9.17%\n",
      "Epoch [7/10], Train Loss: 0.0133, Train Acc: 100.00%, Val Loss: 3.8970, Val Acc: 6.67%\n",
      "Epoch [8/10], Train Loss: 0.0105, Train Acc: 100.00%, Val Loss: 3.8838, Val Acc: 6.67%\n",
      "Epoch [9/10], Train Loss: 0.0090, Train Acc: 100.00%, Val Loss: 3.8732, Val Acc: 8.33%\n",
      "Epoch [10/10], Train Loss: 0.0079, Train Acc: 100.00%, Val Loss: 3.8788, Val Acc: 6.67%\n",
      "Epoch [11/10], Train Loss: 0.0067, Train Acc: 100.00%, Val Loss: 3.8877, Val Acc: 6.67%\n",
      "Epoch [12/10], Train Loss: 0.0059, Train Acc: 100.00%, Val Loss: 3.8997, Val Acc: 6.67%\n",
      "Epoch [13/10], Train Loss: 0.0053, Train Acc: 100.00%, Val Loss: 3.9358, Val Acc: 7.50%\n",
      "Epoch [14/10], Train Loss: 0.0048, Train Acc: 100.00%, Val Loss: 3.9493, Val Acc: 7.50%\n",
      "Epoch [15/10], Train Loss: 0.0042, Train Acc: 100.00%, Val Loss: 3.9626, Val Acc: 6.67%\n",
      "Epoch [16/10], Train Loss: 0.0040, Train Acc: 100.00%, Val Loss: 3.9633, Val Acc: 8.33%\n",
      "Epoch [17/10], Train Loss: 0.0036, Train Acc: 100.00%, Val Loss: 3.9818, Val Acc: 8.33%\n",
      "Epoch [18/10], Train Loss: 0.0031, Train Acc: 100.00%, Val Loss: 4.0236, Val Acc: 6.67%\n",
      "Epoch [19/10], Train Loss: 0.0029, Train Acc: 100.00%, Val Loss: 3.9490, Val Acc: 8.33%\n",
      "Epoch [20/10], Train Loss: 0.0027, Train Acc: 100.00%, Val Loss: 4.0537, Val Acc: 7.50%\n",
      "Average Train Loss: 0.0028, Average Train Acc: 100.00%\n",
      "Average Val Loss: 4.1097, Average Val Acc: 5.98%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Set the number of folds\n",
    "n_folds = 5\n",
    "\n",
    "# Initialize the cross-validation split\n",
    "kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store the results\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "# Iterate through the folds\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(tensor_2)):\n",
    "    print(f'Fold {fold + 1}/{n_folds}')\n",
    "    # Move the model to the device\n",
    "    device = torch.device('mps')\n",
    "    model = ConvNet().to(device=device)\n",
    "\n",
    "    # Create train and validation dataloaders\n",
    "    train_loader = DataLoader(dataset=tensor_2, batch_size=4, sampler=torch.utils.data.SubsetRandomSampler(train_idx))\n",
    "    val_loader = DataLoader(dataset=tensor_2, batch_size=1, sampler=torch.utils.data.SubsetRandomSampler(val_idx))\n",
    "\n",
    "    \n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00003)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(20):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device, dtype=torch.float32), labels.to(device, dtype=torch.long)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += predicted.eq(labels).sum().item()\n",
    "        train_loss = running_train_loss / len(train_loader)\n",
    "        train_acc = 100 * correct_train / total_train\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device, dtype=torch.float32), labels.to(device, dtype=torch.long)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += predicted.eq(labels).sum().item()\n",
    "        val_loss = running_val_loss / len(val_loader)\n",
    "        val_acc = 100 * correct_val / total_val\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{10}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "    # Append the results for the current fold\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "# Calculate the average results across the folds\n",
    "avg_train_loss = sum(train_losses) / n_folds\n",
    "avg_val_loss = sum(val_losses) / n_folds\n",
    "avg_train_acc = sum(train_accs) / n_folds\n",
    "avg_val_acc = sum(val_accs) / n_folds\n",
    "\n",
    "print(f'Average Train Loss: {avg_train_loss:.4f}, Average Train Acc: {avg_train_acc:.2f}%')\n",
    "print(f'Average Val Loss: {avg_val_loss:.4f}, Average Val Acc: {avg_val_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
