{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datahandling import BcomMEG\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reading_data = np.load('/Users/ciprianbangu/Cogmaster/M2 Internship/BCI code/covert_reading_epochs.npy')\n",
    "producing_data = np.load('/Users/ciprianbangu/Cogmaster/M2 Internship/BCI code/covert_producing_epochs.npy')\n",
    "reading_labels = np.zeros(reading_data.shape[0])\n",
    "producing_labels = np.ones(producing_data.shape[0])\n",
    "data = np.concatenate((reading_data, producing_data), axis=0)\n",
    "del reading_data, producing_data\n",
    "labels = np.concatenate((reading_labels, producing_labels), axis=0)\n",
    "del reading_labels, producing_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24644, 247, 241)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MEGSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sequences: list or np.ndarray of shape (N, seq_len, input_size)\n",
    "            labels: list or np.ndarray of shape (N,)\n",
    "            transform: optional transform to apply to each sample\n",
    "        \"\"\"\n",
    "        # normalization\n",
    "        mean = sequences.mean(axis=(0,2), keepdims=True)\n",
    "        std = sequences.std(axis=(0,2), keepdims=True)\n",
    "        sequences = (sequences - mean) / std\n",
    "        \n",
    "        self.sequences = torch.tensor(sequences.transpose(0,2,1), dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.sequences[idx]\n",
    "        y = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MEGru(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout=0.5):\n",
    "        super(MEGru, self).__init__()\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout = dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, h_n = self.gru(x)\n",
    "        out = out[:, -1, :]#h_n[-1] \n",
    "        self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "train_dataset = MEGSequenceDataset(X_train, y_train)\n",
    "test_dataset = MEGSequenceDataset(X_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, criterion, device='mps', log_interval=10):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        if log_interval is not None and (batch_idx + 1) % log_interval == 0:\n",
    "            print(f\"  [Train] Batch {batch_idx+1}/{len(train_loader)} | \"\n",
    "                  f\"Loss: {loss.item():.4f} | Acc: {(predicted == labels).float().mean():.4f}\")\n",
    "\n",
    "    avg_loss = train_loss / total\n",
    "    avg_acc = correct / total\n",
    "    print(f\"[Train] Epoch Done | Avg Loss: {avg_loss:.4f} | Avg Acc: {avg_acc:.4f}\")\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "def eval_model(model, test_loader, criterion, device='mps', log_interval=10):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(test_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            if log_interval is not None and (batch_idx + 1) % log_interval == 0:\n",
    "                print(f\"  [Eval ] Batch {batch_idx+1}/{len(test_loader)} | \"\n",
    "                      f\"Loss: {loss.item():.4f} | Acc: {(predicted == labels).float().mean():.4f}\")\n",
    "\n",
    "        avg_loss = test_loss / total\n",
    "        avg_acc = correct / total\n",
    "        print(f\"[Eval ] Epoch Done | Avg Loss: {avg_loss:.4f} | Avg Acc: {avg_acc:.4f}\")\n",
    "        return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Training GRU with 5 layer(s)\n",
      "  [Train] Batch 10/155 | Loss: 0.6884 | Acc: 0.5312\n",
      "  [Train] Batch 20/155 | Loss: 0.6764 | Acc: 0.5391\n",
      "  [Train] Batch 30/155 | Loss: 0.7026 | Acc: 0.4062\n",
      "  [Train] Batch 40/155 | Loss: 0.6877 | Acc: 0.5469\n",
      "  [Train] Batch 50/155 | Loss: 0.6948 | Acc: 0.5078\n",
      "  [Train] Batch 60/155 | Loss: 0.6980 | Acc: 0.5078\n",
      "  [Train] Batch 70/155 | Loss: 0.6854 | Acc: 0.6094\n",
      "  [Train] Batch 80/155 | Loss: 0.7203 | Acc: 0.3906\n",
      "  [Train] Batch 90/155 | Loss: 0.6883 | Acc: 0.5391\n",
      "  [Train] Batch 100/155 | Loss: 0.7044 | Acc: 0.4922\n",
      "  [Train] Batch 110/155 | Loss: 0.7097 | Acc: 0.4844\n",
      "  [Train] Batch 120/155 | Loss: 0.6940 | Acc: 0.5312\n",
      "  [Train] Batch 130/155 | Loss: 0.6884 | Acc: 0.5391\n",
      "  [Train] Batch 140/155 | Loss: 0.7109 | Acc: 0.5078\n",
      "  [Train] Batch 150/155 | Loss: 0.7058 | Acc: 0.4375\n",
      "[Train] Epoch Done | Avg Loss: 0.6997 | Avg Acc: 0.4943\n",
      "  [Eval ] Batch 10/155 | Loss: 0.6989 | Acc: 0.5625\n",
      "  [Eval ] Batch 20/155 | Loss: 0.7429 | Acc: 0.3125\n",
      "  [Eval ] Batch 30/155 | Loss: 0.6828 | Acc: 0.5625\n",
      "  [Eval ] Batch 40/155 | Loss: 0.7097 | Acc: 0.4375\n",
      "  [Eval ] Batch 50/155 | Loss: 0.7038 | Acc: 0.5000\n",
      "  [Eval ] Batch 60/155 | Loss: 0.7452 | Acc: 0.3125\n",
      "  [Eval ] Batch 70/155 | Loss: 0.7340 | Acc: 0.3438\n",
      "  [Eval ] Batch 80/155 | Loss: 0.6960 | Acc: 0.5625\n",
      "  [Eval ] Batch 90/155 | Loss: 0.6937 | Acc: 0.5625\n",
      "  [Eval ] Batch 100/155 | Loss: 0.6624 | Acc: 0.6875\n",
      "  [Eval ] Batch 110/155 | Loss: 0.7088 | Acc: 0.4688\n",
      "  [Eval ] Batch 120/155 | Loss: 0.6857 | Acc: 0.5625\n",
      "  [Eval ] Batch 130/155 | Loss: 0.7000 | Acc: 0.5312\n",
      "  [Eval ] Batch 140/155 | Loss: 0.7157 | Acc: 0.4062\n",
      "  [Eval ] Batch 150/155 | Loss: 0.7073 | Acc: 0.4688\n",
      "[Eval ] Epoch Done | Avg Loss: 0.6989 | Avg Acc: 0.5038\n",
      "Epoch  1 | Train Acc: 0.4943 | Test Acc: 0.5038\n",
      "  [Train] Batch 10/155 | Loss: 0.6820 | Acc: 0.5391\n",
      "  [Train] Batch 20/155 | Loss: 0.7670 | Acc: 0.4453\n",
      "  [Train] Batch 30/155 | Loss: 0.6892 | Acc: 0.5234\n",
      "  [Train] Batch 40/155 | Loss: 0.6936 | Acc: 0.5625\n",
      "  [Train] Batch 50/155 | Loss: 0.7020 | Acc: 0.4844\n",
      "  [Train] Batch 60/155 | Loss: 0.7068 | Acc: 0.4688\n",
      "  [Train] Batch 70/155 | Loss: 0.7381 | Acc: 0.4375\n",
      "  [Train] Batch 80/155 | Loss: 0.6728 | Acc: 0.6328\n",
      "  [Train] Batch 90/155 | Loss: 0.7124 | Acc: 0.4844\n",
      "  [Train] Batch 100/155 | Loss: 0.7096 | Acc: 0.5156\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m best_test_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):  \u001b[38;5;66;03m# or whatever number of epochs\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m train_model(model, train_loader, optimizer, criterion, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     17\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m eval_model(model, test_loader, criterion, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m test_acc \u001b[38;5;241m>\u001b[39m best_test_acc:\n",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, optimizer, criterion, device, log_interval)\u001b[0m\n\u001b[1;32m     11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/neuroai/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/neuroai/lib/python3.12/site-packages/torch/autograd/__init__.py:288\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    283\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m _engine_run_backward(\n\u001b[1;32m    289\u001b[0m     tensors,\n\u001b[1;32m    290\u001b[0m     grad_tensors_,\n\u001b[1;32m    291\u001b[0m     retain_graph,\n\u001b[1;32m    292\u001b[0m     create_graph,\n\u001b[1;32m    293\u001b[0m     inputs,\n\u001b[1;32m    294\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    295\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    296\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/neuroai/lib/python3.12/site-packages/torch/autograd/graph.py:799\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    797\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    800\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    801\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    803\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layer_results = {}\n",
    "channels = 247\n",
    "num_classes = 2\n",
    "device = \"mps\"\n",
    "\n",
    "for num_layers in [5]:\n",
    "    print(f\"\\nüîÅ Training GRU with {num_layers} layer(s)\")\n",
    "    \n",
    "    model = MEGru(input_size=channels, hidden_size=128, output_size=num_classes, num_layers=num_layers).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-3, weight_decay=1e-5)\n",
    "\n",
    "    best_test_acc = 0.0\n",
    "\n",
    "    for epoch in range(10):  # or whatever number of epochs\n",
    "        train_loss, train_acc = train_model(model, train_loader, optimizer, criterion, device=device)\n",
    "        test_loss, test_acc = eval_model(model, test_loader, criterion, device=device)\n",
    "\n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "        print(f\"Epoch {epoch+1:2d} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "    layer_results[num_layers] = best_test_acc\n",
    "\n",
    "print(\"\\n Best Accuracy by Layer Count:\")\n",
    "for n_layers, acc in layer_results.items():\n",
    "    print(f\"  {n_layers} layer(s): {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_dataset = torch.utils.data.Subset(train_dataset, range(1000))\n",
    "tiny_loader = torch.utils.data.DataLoader(tiny_dataset, batch_size=20, shuffle=True)\n",
    "tiny_test = torch.utils.data.Subset(test_dataset, range(10))\n",
    "tiny_test_loader = torch.utils.data.DataLoader(tiny_test, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Training GRU with 2 layer(s)\n",
      "  [Train] Batch 10/50 | Loss: 0.6849 | Acc: 0.6000\n",
      "  [Train] Batch 20/50 | Loss: 0.6503 | Acc: 0.7500\n",
      "  [Train] Batch 30/50 | Loss: 0.6862 | Acc: 0.6500\n",
      "  [Train] Batch 40/50 | Loss: 0.6443 | Acc: 0.7000\n",
      "  [Train] Batch 50/50 | Loss: 0.7922 | Acc: 0.3000\n",
      "[Train] Epoch Done | Avg Loss: 0.7234 | Avg Acc: 0.4940\n",
      "[Eval ] Epoch Done | Avg Loss: 0.7326 | Avg Acc: 0.6000\n",
      "Epoch  1 | Train Acc: 0.4940 | Test Acc: 0.6000\n",
      "  [Train] Batch 10/50 | Loss: 0.7166 | Acc: 0.4500\n",
      "  [Train] Batch 20/50 | Loss: 0.6801 | Acc: 0.6000\n",
      "  [Train] Batch 30/50 | Loss: 0.6668 | Acc: 0.5500\n",
      "  [Train] Batch 40/50 | Loss: 0.7000 | Acc: 0.5500\n",
      "  [Train] Batch 50/50 | Loss: 0.7325 | Acc: 0.5000\n",
      "[Train] Epoch Done | Avg Loss: 0.6953 | Avg Acc: 0.5230\n",
      "[Eval ] Epoch Done | Avg Loss: 0.7690 | Avg Acc: 0.4000\n",
      "Epoch  2 | Train Acc: 0.5230 | Test Acc: 0.4000\n",
      "  [Train] Batch 10/50 | Loss: 0.6137 | Acc: 0.7000\n",
      "  [Train] Batch 20/50 | Loss: 0.7269 | Acc: 0.5500\n",
      "  [Train] Batch 30/50 | Loss: 0.6376 | Acc: 0.6000\n",
      "  [Train] Batch 40/50 | Loss: 0.6497 | Acc: 0.6500\n",
      "  [Train] Batch 50/50 | Loss: 0.7635 | Acc: 0.4500\n",
      "[Train] Epoch Done | Avg Loss: 0.6807 | Avg Acc: 0.5710\n",
      "[Eval ] Epoch Done | Avg Loss: 0.8415 | Avg Acc: 0.3000\n",
      "Epoch  3 | Train Acc: 0.5710 | Test Acc: 0.3000\n",
      "  [Train] Batch 10/50 | Loss: 0.6037 | Acc: 0.7000\n",
      "  [Train] Batch 20/50 | Loss: 0.6584 | Acc: 0.6000\n",
      "  [Train] Batch 30/50 | Loss: 0.7464 | Acc: 0.6000\n",
      "  [Train] Batch 40/50 | Loss: 0.6515 | Acc: 0.6500\n",
      "  [Train] Batch 50/50 | Loss: 0.7734 | Acc: 0.5000\n",
      "[Train] Epoch Done | Avg Loss: 0.6397 | Avg Acc: 0.6390\n",
      "[Eval ] Epoch Done | Avg Loss: 0.8621 | Avg Acc: 0.3000\n",
      "Epoch  4 | Train Acc: 0.6390 | Test Acc: 0.3000\n",
      "  [Train] Batch 10/50 | Loss: 0.4668 | Acc: 0.8000\n",
      "  [Train] Batch 20/50 | Loss: 0.6298 | Acc: 0.7000\n",
      "  [Train] Batch 30/50 | Loss: 0.3950 | Acc: 0.7500\n",
      "  [Train] Batch 40/50 | Loss: 0.5774 | Acc: 0.6500\n",
      "  [Train] Batch 50/50 | Loss: 0.6289 | Acc: 0.7500\n",
      "[Train] Epoch Done | Avg Loss: 0.5202 | Avg Acc: 0.7550\n",
      "[Eval ] Epoch Done | Avg Loss: 1.1234 | Avg Acc: 0.5000\n",
      "Epoch  5 | Train Acc: 0.7550 | Test Acc: 0.5000\n",
      "  [Train] Batch 10/50 | Loss: 0.2155 | Acc: 0.9500\n",
      "  [Train] Batch 20/50 | Loss: 0.3205 | Acc: 0.8500\n",
      "  [Train] Batch 30/50 | Loss: 0.2391 | Acc: 0.9500\n",
      "  [Train] Batch 40/50 | Loss: 0.4757 | Acc: 0.7500\n",
      "  [Train] Batch 50/50 | Loss: 0.2354 | Acc: 0.9000\n",
      "[Train] Epoch Done | Avg Loss: 0.3979 | Avg Acc: 0.8370\n",
      "[Eval ] Epoch Done | Avg Loss: 1.1444 | Avg Acc: 0.6000\n",
      "Epoch  6 | Train Acc: 0.8370 | Test Acc: 0.6000\n",
      "  [Train] Batch 10/50 | Loss: 0.1810 | Acc: 0.9500\n",
      "  [Train] Batch 20/50 | Loss: 0.6433 | Acc: 0.8000\n",
      "  [Train] Batch 30/50 | Loss: 0.1651 | Acc: 1.0000\n",
      "  [Train] Batch 40/50 | Loss: 0.4259 | Acc: 0.9000\n",
      "  [Train] Batch 50/50 | Loss: 0.4742 | Acc: 0.7500\n",
      "[Train] Epoch Done | Avg Loss: 0.3308 | Avg Acc: 0.8650\n",
      "[Eval ] Epoch Done | Avg Loss: 1.6052 | Avg Acc: 0.4000\n",
      "Epoch  7 | Train Acc: 0.8650 | Test Acc: 0.4000\n",
      "  [Train] Batch 10/50 | Loss: 0.0917 | Acc: 1.0000\n",
      "  [Train] Batch 20/50 | Loss: 0.0533 | Acc: 1.0000\n",
      "  [Train] Batch 30/50 | Loss: 0.1095 | Acc: 0.9000\n",
      "  [Train] Batch 40/50 | Loss: 0.1962 | Acc: 0.9500\n",
      "  [Train] Batch 50/50 | Loss: 0.4774 | Acc: 0.8000\n",
      "[Train] Epoch Done | Avg Loss: 0.1986 | Avg Acc: 0.9310\n",
      "[Eval ] Epoch Done | Avg Loss: 1.4195 | Avg Acc: 0.6000\n",
      "Epoch  8 | Train Acc: 0.9310 | Test Acc: 0.6000\n",
      "  [Train] Batch 10/50 | Loss: 0.1745 | Acc: 0.9000\n",
      "  [Train] Batch 20/50 | Loss: 0.1344 | Acc: 0.9500\n",
      "  [Train] Batch 30/50 | Loss: 0.1601 | Acc: 0.9500\n",
      "  [Train] Batch 40/50 | Loss: 0.0914 | Acc: 1.0000\n",
      "  [Train] Batch 50/50 | Loss: 0.0301 | Acc: 1.0000\n",
      "[Train] Epoch Done | Avg Loss: 0.1769 | Avg Acc: 0.9400\n",
      "[Eval ] Epoch Done | Avg Loss: 2.0731 | Avg Acc: 0.3000\n",
      "Epoch  9 | Train Acc: 0.9400 | Test Acc: 0.3000\n",
      "  [Train] Batch 10/50 | Loss: 0.0754 | Acc: 1.0000\n",
      "  [Train] Batch 20/50 | Loss: 0.0910 | Acc: 0.9500\n",
      "  [Train] Batch 30/50 | Loss: 0.2735 | Acc: 0.9000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m best_test_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):  \u001b[38;5;66;03m# or whatever number of epochs\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m train_model(model, tiny_loader, optimizer, criterion, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     17\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m eval_model(model, tiny_test_loader, criterion, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m test_acc \u001b[38;5;241m>\u001b[39m best_test_acc:\n",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, optimizer, criterion, device, log_interval)\u001b[0m\n\u001b[1;32m      8\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/neuroai/lib/python3.12/site-packages/torch/nn/modules/module.py:1716\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1716\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/neuroai/lib/python3.12/site-packages/torch/nn/modules/module.py:1727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1725\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1726\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1730\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 16\u001b[0m, in \u001b[0;36mMEGru.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 16\u001b[0m     out, h_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgru(x)\n\u001b[1;32m     17\u001b[0m     out \u001b[38;5;241m=\u001b[39m h_n[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m#out[:, -1, :]\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(out)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/neuroai/lib/python3.12/site-packages/torch/nn/modules/module.py:1716\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1716\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/neuroai/lib/python3.12/site-packages/torch/nn/modules/module.py:1727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1725\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1726\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1730\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/neuroai/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1391\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1391\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\n\u001b[1;32m   1392\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1393\u001b[0m         hx,\n\u001b[1;32m   1394\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights,\n\u001b[1;32m   1395\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   1396\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m   1397\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout,\n\u001b[1;32m   1398\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[1;32m   1399\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m   1400\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first,\n\u001b[1;32m   1401\u001b[0m     )\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1403\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\n\u001b[1;32m   1404\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1405\u001b[0m         batch_sizes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1412\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m   1413\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layer_results = {}\n",
    "channels = 247\n",
    "num_classes = 2\n",
    "device = \"mps\"\n",
    "\n",
    "for num_layers in [2, 3, 4]:\n",
    "    print(f\"\\nüîÅ Training GRU with {num_layers} layer(s)\")\n",
    "    \n",
    "    model = MEGru(input_size=channels, hidden_size=128, output_size=num_classes, num_layers=num_layers).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "\n",
    "    best_test_acc = 0.0\n",
    "\n",
    "    for epoch in range(10):  # or whatever number of epochs\n",
    "        train_loss, train_acc = train_model(model, tiny_loader, optimizer, criterion, device=device)\n",
    "        test_loss, test_acc = eval_model(model, tiny_test_loader, criterion, device=device)\n",
    "\n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "        print(f\"Epoch {epoch+1:2d} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "    layer_results[num_layers] = best_test_acc\n",
    "\n",
    "print(\"\\n Best Accuracy by Layer Count:\")\n",
    "for n_layers, acc in layer_results.items():\n",
    "    print(f\"  {n_layers} layer(s): {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reading_data = np.load('/Users/ciprianbangu/Cogmaster/M2 Internship/BCI code/covert_reading_epochs.npy')\n",
    "producing_data = np.load('/Users/ciprianbangu/Cogmaster/M2 Internship/BCI code/covert_producing_epochs.npy')\n",
    "reading_labels = np.zeros(reading_data.shape[0])\n",
    "producing_labels = np.ones(producing_data.shape[0])\n",
    "data = np.concatenate((reading_data, producing_data), axis=0)\n",
    "data = data.reshape(data.shape[0], -1)\n",
    "del reading_data, producing_data\n",
    "labels = np.concatenate((reading_labels, producing_labels), axis=0)\n",
    "del reading_labels, producing_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"done scaling\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=300)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='linear', C=1.0, gamma='scale', random_state=42)\n",
    "svm.fit(X_train_pca, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "y_pred = svm.predict(X_test_pca)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def adf_test_all_channels(data, channel_names=None, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Runs ADF test on each channel (flattened across epochs) and plots the p-values.\n",
    "\n",
    "    Parameters:\n",
    "        data : ndarray\n",
    "            Shape (N, C, T) ‚Äî epochs x channels x time\n",
    "        channel_names : list of str, optional\n",
    "            Names of channels. If None, will use integers.\n",
    "        alpha : float\n",
    "            Significance level (default = 0.05)\n",
    "\n",
    "    Returns:\n",
    "        p_values : ndarray of shape (C,)\n",
    "            ADF p-values for each channel\n",
    "    \"\"\"\n",
    "    N, C, T = data.shape\n",
    "    p_values = []\n",
    "\n",
    "    if channel_names is None:\n",
    "        channel_names = [f\"Ch {i}\" for i in range(C)]\n",
    "\n",
    "    for c in range(C):\n",
    "        signal = data[:, c, :].reshape(-1)[::10]  # flatten across epochs\n",
    "        try:\n",
    "            _, pval, *_ = adfuller(signal)\n",
    "        except Exception as e:\n",
    "            print(f\"ADF failed on channel {c}: {e}\")\n",
    "            pval = np.nan\n",
    "        p_values.append(pval)\n",
    "\n",
    "    p_values = np.array(p_values)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.bar(range(C), p_values, tick_label=channel_names)\n",
    "    plt.axhline(y=alpha, color='r', linestyle='--', label=f'p = {alpha}')\n",
    "    plt.ylabel(\"ADF p-value\")\n",
    "    plt.xlabel(\"Channel\")\n",
    "    plt.title(\"ADF Test per Channel (Lower = More Stationary)\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "reading_data = np.load('/Users/ciprianbangu/Cogmaster/M2 Internship/BCI code/covert_reading_epochs.npy')\n",
    "producing_data = np.load('/Users/ciprianbangu/Cogmaster/M2 Internship/BCI code/covert_producing_epochs.npy')\n",
    "print(\"data loaded\")\n",
    "# reading_labels = np.zeros(reading_data.shape[0])\n",
    "# producing_labels = np.ones(producing_data.shape[0])\n",
    "data = np.concatenate((reading_data, producing_data), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del reading_data, producing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m p_vals \u001b[38;5;241m=\u001b[39m adf_test_all_channels(data)\n",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m, in \u001b[0;36madf_test_all_channels\u001b[0;34m(data, channel_names, alpha)\u001b[0m\n\u001b[1;32m     28\u001b[0m signal \u001b[38;5;241m=\u001b[39m data[:, c, :]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[::\u001b[38;5;241m10\u001b[39m]  \u001b[38;5;66;03m# flatten across epochs\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     _, pval, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m adfuller(signal)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mADF failed on channel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/neuroai/lib/python3.12/site-packages/statsmodels/tsa/stattools.py:326\u001b[0m, in \u001b[0;36madfuller\u001b[0;34m(x, maxlag, regression, autolag, store, regresults)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# 1 for level\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# search for lag length with smallest information criteria\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# Note: use the same number of observations to have comparable IC\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# aic and bic: smaller is better\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m regresults:\n\u001b[0;32m--> 326\u001b[0m     icbest, bestlag \u001b[38;5;241m=\u001b[39m _autolag(\n\u001b[1;32m    327\u001b[0m         OLS, xdshort, fullRHS, startlag, maxlag, autolag\n\u001b[1;32m    328\u001b[0m     )\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     icbest, bestlag, alres \u001b[38;5;241m=\u001b[39m _autolag(\n\u001b[1;32m    331\u001b[0m         OLS,\n\u001b[1;32m    332\u001b[0m         xdshort,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m         regresults\u001b[38;5;241m=\u001b[39mregresults,\n\u001b[1;32m    338\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/neuroai/lib/python3.12/site-packages/statsmodels/tsa/stattools.py:133\u001b[0m, in \u001b[0;36m_autolag\u001b[0;34m(mod, endog, exog, startlag, maxlag, method, modargs, fitargs, regresults)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(startlag, startlag \u001b[38;5;241m+\u001b[39m maxlag \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    132\u001b[0m     mod_instance \u001b[38;5;241m=\u001b[39m mod(endog, exog[:, :lag], \u001b[38;5;241m*\u001b[39mmodargs)\n\u001b[0;32m--> 133\u001b[0m     results[lag] \u001b[38;5;241m=\u001b[39m mod_instance\u001b[38;5;241m.\u001b[39mfit()\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maic\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    136\u001b[0m     icbest, bestlag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m((v\u001b[38;5;241m.\u001b[39maic, k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/neuroai/lib/python3.12/site-packages/statsmodels/regression/linear_model.py:333\u001b[0m, in \u001b[0;36mRegressionModel.fit\u001b[0;34m(self, method, cov_type, cov_kwds, use_t, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpinv\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpinv_wexog\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalized_cov_params\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    331\u001b[0m             \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m--> 333\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpinv_wexog, singular_values \u001b[38;5;241m=\u001b[39m pinv_extended(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwexog)\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalized_cov_params \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpinv_wexog, np\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpinv_wexog))\n\u001b[1;32m    337\u001b[0m         \u001b[38;5;66;03m# Cache these singular values for use later.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/neuroai/lib/python3.12/site-packages/statsmodels/tools/tools.py:264\u001b[0m, in \u001b[0;36mpinv_extended\u001b[0;34m(x, rcond)\u001b[0m\n\u001b[1;32m    262\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\n\u001b[1;32m    263\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mconjugate()\n\u001b[0;32m--> 264\u001b[0m u, s, vt \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39msvd(x, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    265\u001b[0m s_orig \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(s)\n\u001b[1;32m    266\u001b[0m m \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/neuroai/lib/python3.12/site-packages/numpy/linalg/linalg.py:1681\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[1;32m   1678\u001b[0m         gufunc \u001b[38;5;241m=\u001b[39m _umath_linalg\u001b[38;5;241m.\u001b[39msvd_n_s\n\u001b[1;32m   1680\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->DdD\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->ddd\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1681\u001b[0m u, s, vh \u001b[38;5;241m=\u001b[39m gufunc(a, signature\u001b[38;5;241m=\u001b[39msignature, extobj\u001b[38;5;241m=\u001b[39mextobj)\n\u001b[1;32m   1682\u001b[0m u \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1683\u001b[0m s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mastype(_realType(result_t), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "p_vals = adf_test_all_channels(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
